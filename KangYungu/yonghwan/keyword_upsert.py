import os
import re
import time
import hashlib
from typing import Any, List, Tuple, Dict

import pandas as pd
from dotenv import load_dotenv
from tqdm import tqdm
from pinecone import Pinecone, ServerlessSpec
from openai import OpenAI
from langchain_core.documents import Document

# .env ë¡œë“œ
load_dotenv()

# ------------------------------
# ìœ í‹¸
# ------------------------------
def _norm(s: Any) -> str:
    """ê³µë°±/ì†Œë¬¸ì/íŠ¸ë¦¼/ë‚´ë¶€ê³µë°± ì •ê·œí™”."""
    s = "" if s is None else str(s)
    s = s.strip().lower()
    s = re.sub(r"\s+", " ", s)
    return s

def make_stable_id_from_content(content: str) -> str:
    """ë‚´ìš© ë¬¸ìì—´ì—ì„œ ì•ˆì •ì  ë²¡í„° ID ìƒì„± (SHA1 20ì)."""
    digest = hashlib.sha1(content.encode("utf-8")).hexdigest()[:20]
    return f"keyword_{digest}"

class KeywordVectorUploader:
    def __init__(self):
        """Pinecone / OpenAI ì´ˆê¸°í™” + ê¸°ë³¸ ì„¤ì •."""
        self.pinecone_api_key = os.getenv("PINECONE_API_KEY")
        self.openai_api_key = os.getenv("OPENAI_API_KEY")

        if not self.pinecone_api_key:
            raise ValueError("âŒ PINECONE_API_KEYê°€ .envì— ì—†ìŠµë‹ˆë‹¤.")
        if not self.openai_api_key:
            raise ValueError("âŒ OPENAI_API_KEYê°€ .envì— ì—†ìŠµë‹ˆë‹¤.")

        print("âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ì™„ë£Œ")

        # Pinecone
        try:
            self.pc = Pinecone(api_key=self.pinecone_api_key)
            print("âœ… Pinecone í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            raise ValueError(f"âŒ Pinecone ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

        # OpenAI
        try:
            self.openai = OpenAI(api_key=self.openai_api_key)
            print("âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            raise ValueError(f"âŒ OpenAI ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

        # ì„¤ì •
        self.index_name = "keyword-vectordb"
        self.dimension = 1536
        self.embedding_model = "text-embedding-3-small"
        self.namespace = ""  # í•„ìš” ì‹œ ë¶„ë¦¬ ì‚¬ìš©
        # ë°°ì¹˜ í¬ê¸°
        self.embed_batch_size = 128
        self.upsert_batch_size = 100

    # ------------------------------
    # ì¸ë±ìŠ¤ ì¬ìƒì„± (ìˆìœ¼ë©´ ì‚­ì œ â†’ ìƒˆë¡œ ìƒì„±)
    # ------------------------------
    def recreate_index(self) -> None:
        """ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ë©´ ì‚­ì œí•˜ê³  ë™ì¼ ìŠ¤í™ìœ¼ë¡œ ì¬ìƒì„±."""
        try:
            names = [idx.name for idx in self.pc.list_indexes()]
            if self.index_name in names:
                print(f"ğŸ—‘ï¸ ê¸°ì¡´ ì¸ë±ìŠ¤ '{self.index_name}' ì‚­ì œ ì¤‘...")
                self.pc.delete_index(self.index_name)
                time.sleep(2)
                print("âœ… ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚­ì œ ì™„ë£Œ")
            print(f"ğŸ”¨ ì¸ë±ìŠ¤ '{self.index_name}' ìƒì„± ì¤‘...")
            self.pc.create_index(
                name=self.index_name,
                dimension=self.dimension,
                metric="cosine",
                spec=ServerlessSpec(cloud="aws", region="us-east-1"),
            )
            print(f"âœ… ì¸ë±ìŠ¤ '{self.index_name}' ìƒì„± ì™„ë£Œ")
            self.wait_until_ready()
        except Exception as e:
            raise ValueError(f"âŒ ì¸ë±ìŠ¤ ì¬ìƒì„± ì‹¤íŒ¨: {e}")

    def wait_until_ready(self, timeout_sec: int = 10, interval_sec: float = 1.0) -> None:
        """ì¸ë±ìŠ¤ê°€ ready ë  ë•Œê¹Œì§€ í´ë§ (ìµœëŒ€ 10ì´ˆ)."""
        print(f"â³ ì¸ë±ìŠ¤ ì¤€ë¹„ ìƒíƒœ í™•ì¸ ì¤‘ (ìµœëŒ€ {timeout_sec}ì´ˆ)...")
        start = time.time()
        while True:
            try:
                desc = self.pc.describe_index(self.index_name)
                status = getattr(desc, "status", {}) or {}
                ready = False
                if isinstance(status, dict):
                    ready = bool(status.get("ready")) or (status.get("state") == "Ready")
                if ready:
                    print("âœ… ì¸ë±ìŠ¤ ì¤€ë¹„ ì™„ë£Œ")
                    return
            except Exception:
                pass
            if time.time() - start > timeout_sec:
                print("âš ï¸ ì¤€ë¹„ í™•ì¸ íƒ€ì„ì•„ì›ƒ ë„ë‹¬(ê³„ì† ì§„í–‰)")
                return
            time.sleep(interval_sec)

    # ------------------------------
    # CSV â†’ Document ë¦¬ìŠ¤íŠ¸
    # ------------------------------
    def create_key_value_content(self, row: pd.Series, columns: List[str]) -> str:
        """ëª¨ë“  ì»¬ëŸ¼ì„ key:value í˜•íƒœë¡œ ì´ì–´ë¶™ì¸ content ìƒì„±."""
        parts = []
        for col in columns:
            val = row.get(col, "")
            if pd.isna(val):
                continue
            sval = str(val).strip()
            if not sval or sval.lower() == "nan":
                continue
            parts.append(f"{col}: {sval}")
        return " | ".join(parts)

    def csv_to_documents(self, csv_path: str) -> List[Document]:
        """CSVë¥¼ LangChain Document ë°°ì—´ë¡œ ë³€í™˜ (ì•ˆì •ì  ID í¬í•¨)."""
        if not os.path.exists(csv_path):
            raise FileNotFoundError(f"âŒ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
        print(f"ğŸ“– CSV ë¡œë”©: {csv_path}")
        df = pd.read_csv(csv_path)
        cols = df.columns.tolist()
        print(f"ğŸ“Š í–‰ {len(df)}ê°œ, ì»¬ëŸ¼ {len(cols)}ê°œ")
        print(f"ğŸ“ ì»¬ëŸ¼: {cols}")

        docs: List[Document] = []
        for _, row in tqdm(df.iterrows(), total=len(df), desc="ğŸ”„ Document ìƒì„±"):
            content = self.create_key_value_content(row, cols)
            if not content.strip():
                continue
            rid = make_stable_id_from_content(_norm(content))
            docs.append(Document(page_content=content, metadata={"id": rid}))
        print(f"âœ… Document {len(docs)}ê°œ ìƒì„± ì™„ë£Œ")
        return docs

    def show_sample_documents(self, documents: List[Document], n: int = 3) -> None:
        print("\n" + "=" * 80)
        print("ğŸ“‹ Document ìƒ˜í”Œ")
        print("=" * 80)
        for i in range(min(n, len(documents))):
            d = documents[i]
            print(f"\n[{i+1}] ID: {d.metadata['id']}")
            print(f"Content: {d.page_content[:300]}")
            print("-" * 60)
        print("=" * 80 + "\n")

    # ------------------------------
    # ë°°ì¹˜ ì„ë² ë”©
    # ------------------------------
    def embed_batch(self, texts: List[str]) -> List[List[float]]:
        resp = self.openai.embeddings.create(model=self.embedding_model, input=texts)
        return [item.embedding for item in resp.data]  # ìˆœì„œ ë³´ì¥

    def documents_to_vectors_batched(self, docs: List[Document]) -> List[Dict]:
        vectors: List[Dict] = []
        print(f"ğŸ”„ ì„ë² ë”©(ë°°ì¹˜) ìƒì„±: batch={self.embed_batch_size}")
        for i in tqdm(range(0, len(docs), self.embed_batch_size), desc="ğŸ§® ì„ë² ë”© ë°°ì¹˜"):
            batch_docs = docs[i : i + self.embed_batch_size]
            texts = [d.page_content for d in batch_docs]
            try:
                embs = self.embed_batch(texts)
                for d, emb in zip(batch_docs, embs):
                    meta = dict(d.metadata)
                    # Pinecone ì½˜ì†”ì—ì„œ text í™•ì¸ ê°€ëŠ¥í•˜ê²Œ ì¶”ê°€
                    meta["text"] = d.page_content
                    vectors.append({
                        "id": meta["id"],
                        "values": emb,
                        "metadata": meta
                    })
            except Exception as e:
                print(f"âš ï¸ ì„ë² ë”© ë°°ì¹˜ ì‹¤íŒ¨ (i={i}): {e}")
                continue
        print(f"âœ… ë²¡í„° {len(vectors)}ê°œ ìƒì„± ì™„ë£Œ")
        return vectors

    # ------------------------------
    # ì—…ì„œíŠ¸(ë°°ì¹˜)
    # ------------------------------
    def upsert_vectors_batched(self, vectors: List[Dict]) -> Tuple[int, int]:
        if not vectors:
            return 0, 0
        index = self.pc.Index(self.index_name)
        ok, ng = 0, 0
        print(f"ğŸ“¤ ì—…ì„œíŠ¸(ë°°ì¹˜): batch={self.upsert_batch_size}")
        for i in tqdm(range(0, len(vectors), self.upsert_batch_size), desc="ğŸ“¦ ì—…ì„œíŠ¸"):
            batch = vectors[i : i + self.upsert_batch_size]
            try:
                res = index.upsert(vectors=batch, namespace=self.namespace)
                if hasattr(res, "upserted_count") and isinstance(res.upserted_count, int):
                    ok += res.upserted_count
                else:
                    ok += len(batch)
                time.sleep(0.2)  # ë ˆì´íŠ¸ë¦¬ë°‹ ì—¬ìœ 
            except Exception as e:
                ng += len(batch)
                print(f"âš ï¸ ì—…ì„œíŠ¸ ì‹¤íŒ¨ (i={i}): {e}")
                continue
        return ok, ng

    # ------------------------------
    # ë©”ì¸ í”Œë¡œìš° (ì¸ë±ìŠ¤ ì¬ìƒì„± í›„ ì „ëŸ‰ ì ì¬)
    # ------------------------------
    def run(self, csv_path: str) -> None:
        print("ğŸš€ Keyword ë²¡í„° ì—…ë¡œë“œ ì‹œì‘! (ì¸ë±ìŠ¤ ì¬ìƒì„±)\n")
        # 1) ì¸ë±ìŠ¤ ì¬ìƒì„± (ìˆìœ¼ë©´ ì‚­ì œ â†’ ìƒˆë¡œ ìƒì„±)
        self.recreate_index()
        index = self.pc.Index(self.index_name)

        # 2) CSV â†’ Documents
        docs = self.csv_to_documents(csv_path)
        if not docs:
            print("âŒ ë³€í™˜í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        self.show_sample_documents(docs)

        # 3) Documents â†’ Vectors (ë°°ì¹˜ ì„ë² ë”©)
        vectors = self.documents_to_vectors_batched(docs)
        if not vectors:
            print("âŒ ìƒì„±í•  ë²¡í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # 4) ëª¨ë“  ë²¡í„° ì—…ì„œíŠ¸
        ok, ng = self.upsert_vectors_batched(vectors)
        print(f"âœ… ì—…ì„œíŠ¸ ì™„ë£Œ | ì„±ê³µ: {ok}  ì‹¤íŒ¨: {ng}")

        # 5) ìµœì¢… í†µê³„
        try:
            stats = index.describe_index_stats()
            after = stats.get("total_vector_count", 0)
            print(f"\nğŸ“Š ìµœì¢… ë²¡í„° ìˆ˜: {after}")
        except Exception as e:
            print(f"âš ï¸ ìµœì¢… í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")

        print("ğŸ‰ ì™„ë£Œ!")

def main():
    csv_file = "keyword_dictionary_final.csv"
    try:
        app = KeywordVectorUploader()
        app.run(csv_file)
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    main()
