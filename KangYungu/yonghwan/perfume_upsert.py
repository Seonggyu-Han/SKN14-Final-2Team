import os
import re
import json
import time
import hashlib
from typing import Dict, Any, List, Optional, Tuple

import pandas as pd
from dotenv import load_dotenv
from tqdm import tqdm
from pinecone import Pinecone, ServerlessSpec
from openai import OpenAI
from langchain_core.documents import Document

# =========================================
# .env ë¡œë“œ
# =========================================
load_dotenv()

# =========================================
# ìœ í‹¸
# =========================================
def _norm(s: Any) -> str:
    s = "" if s is None else str(s)
    s = s.strip()
    s = re.sub(r"\s+", " ", s)
    return s

def make_stable_id(brand: str, name: str) -> str:
    """ë¸Œëœë“œ+ì´ë¦„ ê¸°ë°˜ ì•ˆì •ì  ID"""
    base = f"{brand.strip()}::{name.strip()}".lower()
    hid = hashlib.sha1(base.encode("utf-8")).hexdigest()[:16]
    return f"perfume_{hid}"

class PerfumeVectorUploader:
    def __init__(self):
        """Pinecone / OpenAI ì´ˆê¸°í™” & ì„¤ì •"""
        self.pinecone_api_key = os.getenv("PINECONE_API_KEY")
        self.openai_api_key = os.getenv("OPENAI_API_KEY")

        if not self.pinecone_api_key:
            raise ValueError("âŒ PINECONE_API_KEYê°€ .envì— ì—†ìŠµë‹ˆë‹¤.")
        if not self.openai_api_key:
            raise ValueError("âŒ OPENAI_API_KEYê°€ .envì— ì—†ìŠµë‹ˆë‹¤.")

        print("âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ì™„ë£Œ")

        # Pinecone
        try:
            self.pc = Pinecone(api_key=self.pinecone_api_key)
            print("âœ… Pinecone í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            raise ValueError(f"âŒ Pinecone ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

        # OpenAI
        try:
            self.openai = OpenAI(api_key=self.openai_api_key)
            print("âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            raise ValueError(f"âŒ OpenAI ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

        # ===== ì„¤ì • =====
        self.index_name = "perfume-vectordb"
        self.dimension = 1536
        self.embedding_model = "text-embedding-3-small"

        self.namespace = ""   # í•„ìš” ì‹œ ë¶„ë¦¬
        self.embed_batch_size = 128
        self.upsert_batch_size = 100

    # -------------------------------------
    # ì¸ë±ìŠ¤ ì¬ìƒì„± (ì¡´ì¬í•˜ë©´ ì‚­ì œ í›„ ìƒì„±)
    # -------------------------------------
    def recreate_index(self) -> None:
        try:
            names = [idx.name for idx in self.pc.list_indexes()]
            if self.index_name in names:
                print(f"ğŸ§¨ ì¸ë±ìŠ¤ '{self.index_name}' ì‚­ì œ ì¤‘...")
                self.pc.delete_index(self.index_name)

            print(f"ğŸ”¨ ì¸ë±ìŠ¤ '{self.index_name}' ìƒì„± ì¤‘...")
            self.pc.create_index(
                name=self.index_name,
                dimension=self.dimension,
                metric="cosine",
                spec=ServerlessSpec(cloud="aws", region="us-east-1"),
            )
            print(f"âœ… ì¸ë±ìŠ¤ '{self.index_name}' ìƒì„± ì™„ë£Œ")
            self.wait_until_ready()
        except Exception as e:
            raise ValueError(f"âŒ ì¸ë±ìŠ¤ ì¬ìƒì„± ì‹¤íŒ¨: {e}")

    def wait_until_ready(self, timeout_sec: int = 10, interval_sec: float = 1.0) -> None:
        """
        ì¸ë±ìŠ¤ê°€ ready ë  ë•Œê¹Œì§€ ì§§ê²Œ í´ë§.
        - ê¸°ë³¸: ìµœëŒ€ 10ì´ˆ ë™ì•ˆ 1ì´ˆ ê°„ê²©ìœ¼ë¡œ í™•ì¸
        - ê·¸ ì´í›„ì—ëŠ” ê°•ì œë¡œ ì§„í–‰
        """
        print(f"â³ ì¸ë±ìŠ¤ ì¤€ë¹„ ìƒíƒœ í™•ì¸ ì¤‘...(ìµœëŒ€ {timeout_sec}ì´ˆ)")
        start = time.time()
        while True:
            try:
                desc = self.pc.describe_index(self.index_name)
                status = getattr(desc, "status", {}) or {}
                ready = False
                if isinstance(status, dict):
                    ready = bool(status.get("ready")) or (status.get("state") == "Ready")
                if ready:
                    print("âœ… ì¸ë±ìŠ¤ ì¤€ë¹„ ì™„ë£Œ")
                    return
            except Exception:
                pass
            if time.time() - start > timeout_sec:
                print("âš ï¸ ì¤€ë¹„ í™•ì¸ íƒ€ì„ì•„ì›ƒ â†’ ê°•ì œ ì§„í–‰")
                return
            time.sleep(interval_sec)

    # -------------------------------------
    # CSV â†’ Document
    # -------------------------------------
    def parse_score_string(self, score_str: str) -> Optional[str]:
        if pd.isna(score_str) or not str(score_str).strip() or str(score_str).lower() == "nan":
            return None
        try:
            s = str(score_str).strip()
            scores: Dict[str, float] = {}
            if "(" in s and ")" in s:
                pattern = r"(\w+)\s*\(\s*([\d.]+)\s*\)"
                for key, val in re.findall(pattern, s):
                    try:
                        scores[key.strip()] = float(val.strip())
                    except ValueError:
                        continue
            elif s.startswith("{") and s.endswith("}"):
                try:
                    d = json.loads(s)
                    for k, v in d.items():
                        if isinstance(v, str):
                            cv = v.replace("%", "").strip()
                            if cv:
                                scores[str(k)] = float(cv)
                        elif isinstance(v, (int, float)):
                            scores[str(k)] = float(v)
                except json.JSONDecodeError:
                    pass
            return max(scores, key=scores.get) if scores else None
        except Exception:
            return None

    def csv_to_documents(self, csv_path: str) -> List[Document]:
        if not os.path.exists(csv_path):
            raise FileNotFoundError(f"âŒ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")

        print(f"ğŸ“– CSV ë¡œë”©: {csv_path}")
        df = pd.read_csv(csv_path)
        print(f"ğŸ“Š í–‰ {len(df)}ê°œ")

        docs: List[Document] = []
        for _, row in tqdm(df.iterrows(), total=len(df), desc="ğŸ”„ Document ìƒì„±"):
            description = str(row.get("description", "")).strip()
            if not description or description.lower() == "nan":
                continue

            season_top   = self.parse_score_string(str(row.get("season_score", "")))
            daynight_top = self.parse_score_string(str(row.get("day_night_score", "")))

            brand = _norm(row.get("brand", ""))
            name  = _norm(row.get("name", ""))

            meta: Dict[str, Any] = {
                "id": make_stable_id(brand, name),
                "brand": brand,
                "name": name,
                "concentration": _norm(row.get("concentration", "")),
                "gender": _norm(row.get("gender", "")),
                "sizes": _norm(row.get("sizes", "")),
            }
            if season_top:   meta["season_score"] = season_top
            if daynight_top: meta["day_night_score"] = daynight_top

            docs.append(Document(page_content=description, metadata=meta))

        print(f"âœ… Document {len(docs)}ê°œ ìƒì„± ì™„ë£Œ")
        return docs

    def show_sample_documents(self, documents: List[Document], n: int = 3) -> None:
        print("\n" + "=" * 80)
        print("ğŸ“‹ Document ìƒ˜í”Œ")
        print("=" * 80)
        for i in range(min(n, len(documents))):
            d = documents[i]
            print(f"\n[{i+1}] ID: {d.metadata['id']}")
            print(f"page_content: {d.page_content[:300]}")
            print(f"metadata keys: {list(d.metadata.keys())}")
            print("-" * 60)
        print("=" * 80 + "\n")

    # -------------------------------------
    # ë°°ì¹˜ ì„ë² ë”©
    # -------------------------------------
    def embed_batch(self, texts: List[str]) -> List[List[float]]:
        resp = self.openai.embeddings.create(model=self.embedding_model, input=texts)
        return [item.embedding for item in resp.data]

    def documents_to_vectors_batched(self, docs: List[Document]) -> List[Dict]:
        vectors: List[Dict] = []
        print(f"ğŸ”„ ì„ë² ë”©(ë°°ì¹˜) ìƒì„±: batch={self.embed_batch_size}")
        for i in tqdm(range(0, len(docs), self.embed_batch_size), desc="ğŸ§® ì„ë² ë”© ë°°ì¹˜"):
            batch_docs = docs[i : i + self.embed_batch_size]
            texts = [d.page_content for d in batch_docs]
            try:
                embs = self.embed_batch(texts)
                for d, emb in zip(batch_docs, embs):
                    meta = dict(d.metadata)
                    meta["text"] = d.page_content
                    vectors.append({"id": meta["id"], "values": emb, "metadata": meta})
            except Exception as e:
                print(f"âš ï¸ ì„ë² ë”© ë°°ì¹˜ ì‹¤íŒ¨ (i={i}): {e}")
                continue
        print(f"âœ… ë²¡í„° {len(vectors)}ê°œ ìƒì„± ì™„ë£Œ")
        return vectors

    # -------------------------------------
    # ì—…ì„œíŠ¸(ë°°ì¹˜)
    # -------------------------------------
    def upsert_vectors_batched(self, vectors: List[Dict]) -> Tuple[int, int]:
        if not vectors:
            return 0, 0
        index = self.pc.Index(self.index_name)
        ok, ng = 0, 0
        calls = 0
        print(f"ğŸ“¤ ì—…ì„œíŠ¸(ë°°ì¹˜): batch={self.upsert_batch_size}")
        for i in tqdm(range(0, len(vectors), self.upsert_batch_size), desc="ğŸ“¦ ì—…ì„œíŠ¸(batched)"):
            batch = vectors[i : i + self.upsert_batch_size]
            try:
                res = index.upsert(vectors=batch, namespace=self.namespace)
                calls += 1
                if hasattr(res, "upserted_count") and isinstance(res.upserted_count, int):
                    ok += res.upserted_count
                else:
                    ok += len(batch)
            except Exception as e:
                ng += len(batch)
                print(f"âš ï¸ ì—…ì„œíŠ¸ ì‹¤íŒ¨ (i={i}): {e}")
                continue
            print(f"   â†³ call#{calls} batch_size={len(batch)} (ëˆ„ì  ì„±ê³µ={ok}, ì‹¤íŒ¨={ng})")
            time.sleep(0.15)
        print(f"ğŸ“ ì—…ì„œíŠ¸ í˜¸ì¶œìˆ˜: {calls}")
        return ok, ng

    # -------------------------------------
    # ì‹¤í–‰
    # -------------------------------------
    def run(self, csv_path: str) -> None:
        print("ğŸš€ Perfume ë²¡í„° ì—…ë¡œë“œ ì‹œì‘!\n")

        # (1) ì¸ë±ìŠ¤ ì¬ìƒì„±: ì¡´ì¬í•˜ë©´ ì‚­ì œ â†’ ìƒˆë¡œ ìƒì„±
        self.recreate_index()

        # (2) CSVâ†’Documents
        docs = self.csv_to_documents(csv_path)
        if not docs:
            print("âŒ ë³€í™˜í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        self.show_sample_documents(docs)

        # (3) Documentsâ†’Vectors (ë°°ì¹˜ ì„ë² ë”©)
        vectors = self.documents_to_vectors_batched(docs)
        if not vectors:
            print("âŒ ìƒì„±í•  ë²¡í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # (4) Upsert (ë°°ì¹˜)
        ok, ng = self.upsert_vectors_batched(vectors)
        print(f"âœ… ì—…ì„œíŠ¸ ì™„ë£Œ | ì„±ê³µ: {ok}  ì‹¤íŒ¨: {ng}")

        # (5) ìµœì¢… í†µê³„
        try:
            idx = self.pc.Index(self.index_name)
            stats = idx.describe_index_stats()
            after = stats.get("total_vector_count", 0)
            print(f"\nğŸ“Š ìµœì¢… ë²¡í„° ìˆ˜: {after}")
        except Exception as e:
            print(f"âš ï¸ ìµœì¢… í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")

        print("ğŸ‰ ì™„ë£Œ!")

# =========================================
# ë©”ì¸
# =========================================
def main():
    csv_file = "perfume_final.csv"
    try:
        app = PerfumeVectorUploader()
        app.run(csv_file)
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    main()
