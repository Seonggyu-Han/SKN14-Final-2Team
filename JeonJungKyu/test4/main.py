# --- stdlib ---
import json
import os
# --- typing ---
from typing import TypedDict, List, Optional, Dict, Any

# --- env ---
from dotenv import load_dotenv
from config import llm, embeddings, index, MODEL_NAME, pc


# --- langchain / langgraph ---
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langgraph.graph import StateGraph, END

# --- services (ì¿¼ë¦¬ íŒŒì‹±/í•„í„°/ê²€ìƒ‰/ì‘ë‹µ/ê°€ê²© í‚¤ì›Œë“œ) ---
from tools import run_llm_parser
from tools import apply_meta_filters
from tools import query_pinecone
from tools import generate_response
from tools import extract_price_search_keywords

# --- tools (Naver ê°€ê²©/ML ì¶”ì²œ) ---
from tools import price_tool, recommend_perfume_simple
load_dotenv()


SUPERVISOR_SYSTEM_PROMPT = """
You are the "Perfume Recommendation Supervisor (Router)". Analyze the user's query (Korean or English) and route to exactly ONE agent below.

[Agents]
- LLM_parser         : Parses/normalizes multi-facet queries (2+ product facets).
- FAQ_agent          : Perfume knowledge / definitions / differences / general questions.
- human_fallback     : Non-perfume or off-topic queries.
- price_agent        : Price-only intents (cheapest, price, buy, discount, etc.).
- ML_agent           : Single-preference recommendations (mood/season vibe like "fresh summer", "sweet", etc.).

[Facets to detect ("product facets")]
- brand            (e.g., Chanel, Dior, Creed)
- season           (spring/summer/fall/winter; "for summer/winter")
- gender           (male/female/unisex)
- sizes            (volume in ml: 30/50/100 ml)
- day_night_score  (day/night/daily/office/club, etc.)
- concentration    (EDT/EDP/Extrait/Parfum/Cologne)

[Price intent keywords (not exhaustive)]
- Korean: ê°€ê²©, ì–¼ë§ˆ, ê°€ê²©ëŒ€, êµ¬ë§¤, íŒë§¤, í• ì¸, ì–´ë””ì„œ ì‚¬, ë°°ì†¡ë¹„
- English: price, cost, cheapest, buy, purchase, discount

[FAQ examples]
- Differences between EDP vs EDT, note definitions, longevity/projection, brand/line info.

[Single-preference (ML_agent) examples]
- "Recommend a cool perfume for summer", "Recommend a sweet scent", "One citrusy fresh pick"
  (= 0â€“1 of the above facets mentioned; primarily taste/mood/situation).


[Routing rules (priority)]
1) Non-perfume / off-topic â†’ human_fallback
2) Pure price-only intent (no product facets mentioned) â†’ price_agent
   e.g., "í–¥ìˆ˜ ê°€ê²© ì•Œë ¤ì¤˜" â†’ price_agent
3) Count product facets in the query:
   - If facets â‰¥ 2 â†’ LLM_parser (can handle price intent within multi-facet queries)
   - If facets = 1 AND has price intent â†’ LLM_parser (e.g., "ìƒ¤ë„¬ í–¥ìˆ˜ ê°€ê²©")
4) Otherwise (single-topic queries):
   - Pure price query with specific brand/product â†’ price_agent
   - Perfume knowledge/definitions â†’ FAQ_agent
   - Single taste/mood recommendation â†’ ML_agent
5) Tie-breakers:
   - If complex query (multiple aspects) â†’ LLM_parser
   - If pure price intent â†’ price_agent
   - Else: knowledge â†’ FAQ_agent, taste â†’ ML_agent

[Output format]
Return ONLY this JSON (no extra text):
{{
  "next": "<LLM_parser|FAQ_agent|human_fallback|price_agent|ML_agent>",
  "reason": "<one short English sentence>",
  "facet_count": <integer>,
  "facets": {{
    "brand": "<value or null>",
    "season": "<value or null>",
    "gender": "<value or null>",
    "sizes": "<value or null>",
    "day_night_score": "<value or null>",
    "concentration": "<value or null>"
  }},
  "scent_vibe": "<value if detected, else null>",
  "query_intent": "<price|faq|scent_pref|non_perfume|other>"
}}
""".strip()

# ---------- 1) State ----------
class AgentState(TypedDict):
    messages: List[BaseMessage]           # conversation log
    next: Optional[str]                   # routing decision key 
    router_json: Optional[Dict[str, Any]] # parsed JSON from router

router_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", SUPERVISOR_SYSTEM_PROMPT),
        ("user", "{query}")
    ]
)

def supervisor_node(state: AgentState) -> AgentState:
    """Call the router LLM and return parsed JSON + routing target."""
    user_query = None
    for m in reversed(state["messages"]):
        if isinstance(m, HumanMessage):
            user_query = m.content
            break
    if not user_query:
        user_query = "(empty)"

    chain = router_prompt | llm
    ai = chain.invoke({"query": user_query})
    text = ai.content

    # JSON strict parse
    chosen = "human_fallback"
    parsed: Dict[str, Any] = {}
    try:
        parsed = json.loads(text)
        maybe = parsed.get("next")
        if isinstance(maybe, str) and maybe in {"LLM_parser","FAQ_agent","human_fallback","price_agent","ML_agent"}:
            chosen = maybe
    except Exception:
        parsed = {"error": "invalid_json", "raw": text}

    msgs = state["messages"] + [AIMessage(content=text)]
    return {
        "messages": msgs,
        "next": chosen,
        "router_json": parsed
    }

# ---------- 4) Agent Nodes ----------
def LLM_parser_node(state: AgentState) -> AgentState:
    """ì‹¤ì œ RAG íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•˜ëŠ” LLM_parser ë…¸ë“œ + ê°€ê²© ê²€ìƒ‰ í†µí•©"""
    user_query = None
    for m in reversed(state["messages"]):
        if isinstance(m, HumanMessage):
            user_query = m.content
            break
    if not user_query:
        user_query = "(empty)"

    try:
        print(f"ğŸ” LLM_parser ì‹¤í–‰: {user_query}")
        
        # 1ë‹¨ê³„: LLMìœ¼ë¡œ ì¿¼ë¦¬ íŒŒì‹±
        parsed_json = run_llm_parser(user_query)
        if "error" in parsed_json:
            error_msg = f"[LLM_parser] ì¿¼ë¦¬ íŒŒì‹± ì˜¤ë¥˜: {parsed_json['error']}"
            msgs = state["messages"] + [AIMessage(content=error_msg)]
            return {"messages": msgs, "next": None, "router_json": state.get("router_json")}
        
        # 2ë‹¨ê³„: ë©”íƒ€í•„í„° ì ìš©
        filtered_json = apply_meta_filters(parsed_json)
        
        # 3ë‹¨ê³„: ì¿¼ë¦¬ ë²¡í„°í™”
        query_vector = embeddings.embed_query(user_query)
        
        # 4ë‹¨ê³„: Pinecone ê²€ìƒ‰
        search_results = query_pinecone(query_vector, filtered_json, top_k=5)
        
        # 5ë‹¨ê³„: ìµœì¢… ì‘ë‹µ ìƒì„±
        final_response = generate_response(user_query, search_results)
        
        # 6ë‹¨ê³„: ê°€ê²© ì˜ë„ ê°ì§€ ë° ê°€ê²© ì •ë³´ ì¶”ê°€
        price_keywords = ['ê°€ê²©', 'ì–¼ë§ˆ', 'ê°€ê²©ëŒ€', 'êµ¬ë§¤', 'íŒë§¤', 'í• ì¸', 'ì–´ë””ì„œ ì‚¬', 'ë°°ì†¡ë¹„', 'price', 'cost', 'cheapest', 'buy', 'purchase', 'discount']
        has_price_intent = any(keyword in user_query.lower() for keyword in price_keywords)
        
        if has_price_intent:
            # ê²€ìƒ‰ëœ í–¥ìˆ˜ë“¤ë¡œë¶€í„° ê°€ê²© ê²€ìƒ‰ìš© í‚¤ì›Œë“œ ì¶”ì¶œ
            price_search_keywords = extract_price_search_keywords(search_results, user_query, parsed_json)
            
            print(f"ğŸ’° ê°€ê²© ê²€ìƒ‰ í‚¤ì›Œë“œ: {price_search_keywords}")
            print(f"ğŸ” ê²€ìƒ‰ëœ í–¥ìˆ˜ ì •ë³´: {search_results.get('matches', [{}])[0].get('metadata', {}) if search_results.get('matches') else 'No matches'}")
            
            if price_search_keywords and price_search_keywords != "í–¥ìˆ˜":
                try:
                    price_info = price_tool.invoke({"user_query": price_search_keywords})
                    
                    # ê°€ê²© ì •ë³´ë¥¼ ìµœì¢… ì‘ë‹µì— ì¶”ê°€
                    final_response_with_price = f"""{final_response}

---

ğŸ’° **ê°€ê²© ì •ë³´**
{price_info}"""
                except Exception as price_error:
                    print(f"âŒ ê°€ê²© ê²€ìƒ‰ ì˜¤ë¥˜: {price_error}")
                    final_response_with_price = f"""{final_response}

---

ğŸ’° **ê°€ê²© ì •ë³´**
âŒ ê°€ê²© ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."""
            else:
                final_response_with_price = f"""{final_response}

---

ğŸ’° **ê°€ê²© ì •ë³´**
ğŸ” êµ¬ì²´ì ì¸ í–¥ìˆ˜ëª…ì´ í•„ìš”í•©ë‹ˆë‹¤. ìœ„ ì¶”ì²œ í–¥ìˆ˜ë“¤ ì¤‘ ì›í•˜ëŠ” ì œí’ˆëª…ì„ ë‹¤ì‹œ ê²€ìƒ‰í•´ë³´ì„¸ìš”."""
        else:
            final_response_with_price = final_response
        
        # ê²°ê³¼ ìš”ì•½
        summary = f"""[LLM_parser] RAG íŒŒì´í”„ë¼ì¸ ì™„ë£Œ âœ…

ğŸ“Š íŒŒì‹± ê²°ê³¼: {json.dumps(parsed_json, ensure_ascii=False)}
ğŸ” í•„í„°ë§ ê²°ê³¼: {json.dumps(filtered_json, ensure_ascii=False)}
ğŸ¯ ê²€ìƒ‰ëœ í–¥ìˆ˜ ê°œìˆ˜: {len(search_results.get('matches', []))}

ğŸ’¬ ì¶”ì²œ ê²°ê³¼:
{final_response_with_price}"""

        msgs = state["messages"] + [AIMessage(content=summary)]
        return {"messages": msgs, "next": None, "router_json": state.get("router_json")}
        
    except Exception as e:
        error_msg = f"[LLM_parser] RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}"
        print(f"âŒ LLM_parser ì „ì²´ ì˜¤ë¥˜: {e}")
        msgs = state["messages"] + [AIMessage(content=error_msg)]
        return {"messages": msgs, "next": None, "router_json": state.get("router_json")}
    
    
def human_fallback_node(state: AgentState) -> AgentState:
    """í–¥ìˆ˜ ê´€ë ¨ ë³µì¡í•œ ì§ˆë¬¸ì— ëŒ€í•œ ê¸°ë³¸ ì‘ë‹µ"""
    user_query = None
    for m in reversed(state["messages"]):
        if isinstance(m, HumanMessage):
            user_query = m.content
            break
    if not user_query:
        user_query = "(empty)"
    
    fallback_response = (
        f"â“ '{user_query}' ë” ëª…í™•í•œ ì„¤ëª…ì´ í•„ìš”í•©ë‹ˆë‹¤.\n"
        f"ğŸ‘‰ ì§ˆë¬¸ì„ êµ¬ì²´ì ìœ¼ë¡œ ë‹¤ì‹œ ì‘ì„±í•´ ì£¼ì„¸ìš”.\n"
        f"ğŸ’¡ ë˜ëŠ” í–¥ìˆ˜ì— ê´€í•œ ë©‹ì§„ ì§ˆë¬¸ì„ í•´ë³´ì‹œëŠ” ê±´ ì–´ë–¨ê¹Œìš”?"
    )
    
    msgs = state["messages"] + [AIMessage(content=fallback_response)]
    return {"messages": msgs, "next": None, "router_json": state.get("router_json")}

# ---------- 5) ì§ì ‘ ë„êµ¬ í˜¸ì¶œ ë°©ì‹ìœ¼ë¡œ ì—ì´ì „íŠ¸ êµ¬í˜„ ----------
def price_agent_node(state: AgentState) -> AgentState:
    """Price agent - ì§ì ‘ ë„êµ¬ í˜¸ì¶œ"""
    user_query = None
    for m in reversed(state["messages"]):
        if isinstance(m, HumanMessage):
            user_query = m.content
            break
    if not user_query:
        user_query = "(empty)"
    
    try:
        # ì§ì ‘ price_tool í˜¸ì¶œ
        price_result = price_tool.invoke({"user_query": user_query})
        
        # ê²°ê³¼ë¥¼ ë” ìì—°ìŠ¤ëŸ½ê²Œ í¬ë§·íŒ…
        final_answer = f"ğŸ’° **ê°€ê²© ì •ë³´**\n\n{price_result}"
        
        msgs = state["messages"] + [AIMessage(content=final_answer)]
        return {
            "messages": msgs, 
            "next": None, 
            "router_json": state.get("router_json")
        }
    except Exception as e:
        error_msg = f"âŒ ê°€ê²© ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        msgs = state["messages"] + [AIMessage(content=error_msg)]
        return {
            "messages": msgs, 
            "next": None, 
            "router_json": state.get("router_json")
        }

def FAQ_agent_node(state: AgentState) -> AgentState:
    """FAQ agent - LLM ê¸°ë³¸ ì§€ì‹ìœ¼ë¡œ í–¥ìˆ˜ ê´€ë ¨ ì§ˆë¬¸ ë‹µë³€"""
    user_query = None
    for m in reversed(state["messages"]):
        if isinstance(m, HumanMessage):
            user_query = m.content
            break
    if not user_query:
        user_query = "(empty)"
    
    try:
        # LLMì—ê²Œ í–¥ìˆ˜ ì§€ì‹ ì „ë¬¸ê°€ë¡œì„œ ë‹µë³€í•˜ë„ë¡ í”„ë¡¬í”„íŠ¸ ì„¤ì •
        faq_prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ í–¥ìˆ˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ í–¥ìˆ˜ ê´€ë ¨ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ìœ ìš©í•œ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.

ë‹¤ìŒê³¼ ê°™ì€ ì£¼ì œë“¤ì— ëŒ€í•´ ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
- í–¥ìˆ˜ì˜ ì¢…ë¥˜ì™€ ë†ë„ (EDT, EDP, Parfum ë“±)
- í–¥ë£Œì™€ ë…¸íŠ¸ì— ëŒ€í•œ ì„¤ëª…
- ë¸Œëœë“œë³„ íŠ¹ì§•ê³¼ ëŒ€í‘œ í–¥ìˆ˜
- í–¥ìˆ˜ ì‚¬ìš©ë²•ê³¼ ë³´ê´€ë²•
- ê³„ì ˆë³„, ìƒí™©ë³„ í–¥ìˆ˜ ì„ íƒ íŒ
- í–¥ìˆ˜ì˜ ì§€ì†ë ¥ê³¼ í™•ì‚°ë ¥

ë‹µë³€ì€ ì¹œê·¼í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ, ê·¸ë¦¬ê³  ì‹¤ìš©ì ì¸ ì¡°ì–¸ì„ í¬í•¨í•´ì„œ í•´ì£¼ì„¸ìš”."""),
            ("user", "{question}")
        ])
        
        chain = faq_prompt | llm
        result = chain.invoke({"question": user_query})
        
        # ê²°ê³¼ë¥¼ í¬ë§·íŒ…
        final_answer = f"ğŸ“š **í–¥ìˆ˜ ì§€ì‹**\n\n{result.content}"
        
        msgs = state["messages"] + [AIMessage(content=final_answer)]
        return {
            "messages": msgs, 
            "next": None, 
            "router_json": state.get("router_json")
        }
    except Exception as e:
        error_msg = f"âŒ í–¥ìˆ˜ ì§€ì‹ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        msgs = state["messages"] + [AIMessage(content=error_msg)]
        return {
            "messages": msgs, 
            "next": None, 
            "router_json": state.get("router_json")
        }

def ML_agent_node(state: AgentState) -> AgentState:
    """ML agent - recommend_perfume_simple ë„êµ¬ í˜¸ì¶œ í›„, LLMì´ ì„¤ëª…ë¬¸ ìƒì„±ê¹Œì§€ ìˆ˜í–‰"""
    user_query = None
    for m in reversed(state["messages"]):
        if isinstance(m, HumanMessage):
            user_query = m.content
            break
    if not user_query:
        user_query = "(empty)"

    try:
        # 1) ML ë„êµ¬ í˜¸ì¶œ (êµ¬ì¡°í™” ë°ì´í„° ë³´ì¡´)
        ml_result = recommend_perfume_simple.invoke({"user_text": user_query})
        # ì˜ˆìƒ êµ¬ì¡°: {"recommendations": [...], "predicted_labels": [...]}
        ml_json_str = json.dumps(ml_result, ensure_ascii=False)

        # 2) LLMì— ì»¨í…ìŠ¤íŠ¸ë¡œ ì „ë‹¬í•˜ì—¬ ìì—°ì–´ ë‹µë³€ ìƒì„±
        system_prompt = (
            "ë„ˆëŠ” í–¥ìˆ˜ ì¶”ì²œ ì„¤ëª…ê°€ì•¼. ì•„ë˜ JSONì€ ML ëª¨ë¸ì˜ ì¶”ì²œ ê²°ê³¼ì´ë‹ˆ, "
            "ê·¸ ì •ë³´ë§Œ ê·¼ê±°ë¡œ ê°„ê²°í•˜ê³  ì¹œì ˆí•œ í•œêµ­ì–´ ë‹µë³€ì„ ë§Œë“¤ì–´ë¼.\n"
            "- ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ì— ë§ì¶° Top 3ë¥¼ í•µì‹¬ ì´ìœ ì™€ í•¨ê»˜ ìš”ì•½\n"
            "- ì˜ˆì¸¡ëœ í–¥ íŠ¹ì„±ì´ ìˆìœ¼ë©´ í•œ ì¤„ë¡œ ë³´ì—¬ì£¼ê¸°\n"
            "- ë¹„ìŠ·í•œ ëŒ€ì•ˆ 2ê°œ ì •ë„ì™€ ë‹¤ìŒ í–‰ë™(ì˜ˆ: ì‹œì¦Œ/ì‹œê°„/ì§€ì†ë ¥ ì„ í˜¸ ì§ˆë¬¸) ì œì•ˆ\n"
            "- ê³¼ì¥í•˜ê±°ë‚˜ JSONì— ì—†ëŠ” ì‚¬ì‹¤ì€ ì¶”ì¸¡í•˜ì§€ ë§ê¸°"
        )
        human_prompt = (
            f"ì‚¬ìš©ì ì§ˆë¬¸:\n{user_query}\n\n"
            f"ML ì¶”ì²œ JSON:\n```json\n{ml_json_str}\n```"
        )

        llm_out = llm.invoke([SystemMessage(content=system_prompt),
                              HumanMessage(content=human_prompt)])

        # 3) ìµœì¢… ë‹µë³€ì„ ëŒ€í™”ì— ì¶”ê°€
        msgs = state["messages"] + [AIMessage(content=llm_out.content)]
        return {
            "messages": msgs,
            "next": None,
            "router_json": state.get("router_json")
        }

    except Exception as e:
        error_msg = f"âŒ ML ì¶”ì²œ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        msgs = state["messages"] + [AIMessage(content=error_msg)]
        return {
            "messages": msgs,
            "next": None,
            "router_json": state.get("router_json")
        }
# ---------- 7) Build Graph ----------
graph = StateGraph(AgentState)

# ë…¸ë“œ ì¶”ê°€
graph.add_node("supervisor", supervisor_node)
graph.add_node("LLM_parser", LLM_parser_node)
graph.add_node("FAQ_agent", FAQ_agent_node)
graph.add_node("human_fallback", human_fallback_node)
graph.add_node("price_agent", price_agent_node)
graph.add_node("ML_agent", ML_agent_node)

# ì‹œì‘ì  ì„¤ì •
graph.set_entry_point("supervisor")

# ì¡°ê±´ë¶€ ë¼ìš°íŒ… í•¨ìˆ˜
def router_edge(state: AgentState) -> str:
    return state["next"] or "human_fallback"

# ì¡°ê±´ë¶€ ì—£ì§€ ì¶”ê°€ (supervisorì—ì„œ ê° agentë¡œ)
graph.add_conditional_edges(
    "supervisor",
    router_edge,
    {
        "LLM_parser": "LLM_parser",
        "FAQ_agent": "FAQ_agent",
        "human_fallback": "human_fallback",
        "price_agent": "price_agent",
        "ML_agent": "ML_agent",
    },
)

# ê° ì—ì´ì „íŠ¸ì—ì„œ ENDë¡œ ê°€ëŠ” ì—£ì§€ ì¶”ê°€
for node in ["LLM_parser", "FAQ_agent", "human_fallback", "price_agent", "ML_agent"]:
    graph.add_edge(node, END)

# ê·¸ë˜í”„ ì»´íŒŒì¼
app = graph.compile()


TEST_QUERIES = [
    "ì…ìƒë¡œë‘ ì—¬ì„±ìš© 50ml ê²¨ìš¸ìš© í–¥ìˆ˜ ì¶”ì²œí•´ì¤˜.ê°€ê²©ë„ ì•Œë ¤ì¤˜",                 
    "ë””ì˜¬ EDPë¡œ ê°€ì„ ë°¤(ì•¼ê°„)ì— ì“¸ ë§Œí•œ í–¥ìˆ˜ ìˆì–´?",                
    "EDPë‘ EDT ì°¨ì´ê°€ ë­ì•¼?",                                       
    "íƒ‘ë…¸íŠ¸Â·ë¯¸ë“¤ë…¸íŠ¸Â·ë² ì´ìŠ¤ë…¸íŠ¸ê°€ ê°ê° ë¬´ìŠ¨ ëœ»ì´ì•¼?",               
    "ì˜¤ëŠ˜ ì ì‹¬ ë­ ë¨¹ì„ê¹Œ?",                                         
    "ì˜¤ëŠ˜ ì„œìš¸ ë‚ ì”¨ ì–´ë•Œ?",                                         
    "ìƒ¤ë„¬ ë„˜ë²„5 50ml ê°€ê²© ì•Œë ¤ì¤˜.",                               
    "ë””ì˜¬ ì†Œë°”ì¥¬ ê°€ê²© ì–¼ë§ˆì•¼? ì–´ë””ì„œ ì‚¬ëŠ” ê²Œ ì œì¼ ì‹¸?",             
    "ì—¬ë¦„ì— ì‹œì›í•œ í–¥ìˆ˜ ì¶”ì²œí•´ì¤˜.",                                 
    "ë‹¬ë‹¬í•œ í–¥ ì¶”ì²œí•´ì¤˜.",
    "ë°”ë³´ê°™ì€í–¥ ì¶”ì²œí•´ì¤˜"
]

def run_tests():
    for q in TEST_QUERIES:
        print("="*80)
        print("Query:", q)
        init: AgentState = {
            "messages": [HumanMessage(content=q)],
            "next": None,
            "router_json": None
        }
        try:
            out = app.invoke(init)
            ai_msgs = [m for m in out["messages"] if isinstance(m, AIMessage)]
            router_raw = ai_msgs[-2].content if len(ai_msgs) >= 2 else "(no router output)"
            agent_summary = ai_msgs[-1].content if ai_msgs else "(no agent output)"
            print("Router JSON:", router_raw)
            print("Agent summary:", agent_summary)
        except Exception as e:
            print(f"Error processing query: {e}")

def run_single_query(query: str):
    """ë‹¨ì¼ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸"""
    print(f"ğŸ” Query: {query}")
    print("-" * 50)
    
    init: AgentState = {
        "messages": [HumanMessage(content=query)],
        "next": None,
        "router_json": None
    }
    
    try:
        out = app.invoke(init)
        ai_msgs = [m for m in out["messages"] if isinstance(m, AIMessage)]
        
        if len(ai_msgs) >= 2:
            print("ğŸ¤– Router Decision:")
            print(ai_msgs[-2].content)
            print("\nğŸ“ Final Response:")
            print(ai_msgs[-1].content)
        elif len(ai_msgs) == 1:
            print("ğŸ“ Response:")
            print(ai_msgs[-1].content)
        else:
            print("âŒ No response generated")
            
    except Exception as e:
        print(f"âŒ Error: {e}")

if __name__ == "__main__":
    # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
    print("ğŸ”§ í™˜ê²½ ë³€ìˆ˜ í™•ì¸:")
    print(f"OPENAI_API_KEY: {'âœ… ì„¤ì •ë¨' if os.getenv('OPENAI_API_KEY') else 'âŒ ë¯¸ì„¤ì •'}")
    print(f"PINECONE_API_KEY: {'âœ… ì„¤ì •ë¨' if os.getenv('PINECONE_API_KEY') else 'âŒ ë¯¸ì„¤ì •'}")
    print(f"NAVER_CLIENT_ID: {'âœ… ì„¤ì •ë¨' if os.getenv('NAVER_CLIENT_ID') else 'âŒ ë¯¸ì„¤ì •'}")
    print(f"NAVER_CLIENT_SECRET: {'âœ… ì„¤ì •ë¨' if os.getenv('NAVER_CLIENT_SECRET') else 'âŒ ë¯¸ì„¤ì •'}")
    print()
    
    print("ğŸš€ í–¥ìˆ˜ ì¶”ì²œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    print()
    
    # ê°œë³„ í…ŒìŠ¤íŠ¸ìš© í•¨ìˆ˜ ì œê³µ
    print("ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ í•¨ìˆ˜:")
    print("- run_tests(): ëª¨ë“  í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì‹¤í–‰")
    print("- run_single_query('your query'): ë‹¨ì¼ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸")
    print()
if __name__ == "__main__":
    ...
    print("ğŸš€ í–¥ìˆ˜ ì¶”ì²œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    print()
    
    # ê°œë³„ í…ŒìŠ¤íŠ¸ìš© í•¨ìˆ˜ ì œê³µ
    print("ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ í•¨ìˆ˜:")
    print("- run_tests(): ëª¨ë“  í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì‹¤í–‰")
    print("- run_single_query('your query'): ë‹¨ì¼ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸")
    print()

    # ğŸ”½ ì´ ë¶€ë¶„ ì¶”ê°€
    run_tests()  
