{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a03d90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Device] cpu\n",
      "í–¥ìˆ˜ ë©”ì¸ ì–´ì½”ë“œ ì¶”ì¶œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸\n",
      "============================================================\n",
      "Model Path: ./model.pkl\n",
      "HF Model: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "Device: cpu\n",
      "Max Length: 256\n",
      "Batch Size: 16\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'inspect_model_bundle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 486\u001b[39m\n\u001b[32m    484\u001b[39m     interactive_test()\n\u001b[32m    485\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m486\u001b[39m     \u001b[43mrun_tests\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 311\u001b[39m, in \u001b[36mrun_tests\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    308\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ í•¨ìˆ˜\"\"\"\u001b[39;00m\n\u001b[32m    310\u001b[39m \u001b[38;5;66;03m# ë¨¼ì € ëª¨ë¸ êµ¬ì¡° ë¶„ì„\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m \u001b[43minspect_model_bundle\u001b[49m(MODEL_PKL)\n\u001b[32m    313\u001b[39m \u001b[38;5;66;03m# í–¥ìˆ˜ ê´€ë ¨ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë“¤ (ë©”ì¸ ì–´ì½”ë“œ ì¶”ì¶œìš©)\u001b[39;00m\n\u001b[32m    314\u001b[39m test_queries = [\n\u001b[32m    315\u001b[39m     \u001b[38;5;66;03m# ê³„ì ˆë³„ í–¥ìˆ˜ ì¶”ì²œ\u001b[39;00m\n\u001b[32m    316\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mì—¬ë¦„ì— ë¿Œë¦´ë§Œí•œ í–¥ìˆ˜ ì¶”ì²œí•´ì£¼ì„¸ìš”\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    351\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mí—ˆë¸Œí–¥ì´ ë“¤ì–´ê°„ ìì—°ìŠ¤ëŸ¬ìš´ í–¥ìˆ˜ ì¶”ì²œ\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    352\u001b[39m ]\n",
      "\u001b[31mNameError\u001b[39m: name 'inspect_model_bundle' is not defined"
     ]
    }
   ],
   "source": [
    "def create_manual_labels():\n",
    "    \"\"\"ìˆ˜ë™ìœ¼ë¡œ í–¥ìˆ˜ ì–´ì½”ë“œ ë¼ë²¨ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ (ì°¸ê³ ìš©)\"\"\"\n",
    "    \n",
    "    # ì¼ë°˜ì ì¸ í–¥ìˆ˜ ì–´ì½”ë“œë“¤ (45ê°œ í´ë˜ìŠ¤ ì˜ˆì‹œ)\n",
    "    fragrance_accords = [\n",
    "        \"Citrus\", \"Fresh\", \"Aquatic\", \"Green\", \"Herbal\",           # 0-4: ìƒì¾Œí•œ ê³„ì—´\n",
    "        \"Floral\", \"Rose\", \"Jasmine\", \"Lily\", \"Peony\",             # 5-9: í”Œë¡œëŸ´ ê³„ì—´  \n",
    "        \"Fruity\", \"Apple\", \"Pear\", \"Peach\", \"Berry\",              # 10-14: ê³¼ì¼ ê³„ì—´\n",
    "        \"Spicy\", \"Pepper\", \"Cinnamon\", \"Ginger\", \"Cardamom\",      # 15-19: ìŠ¤íŒŒì´ì‹œ ê³„ì—´\n",
    "        \"Woody\", \"Sandalwood\", \"Cedar\", \"Pine\", \"Vetiver\",        # 20-24: ìš°ë”” ê³„ì—´\n",
    "        \"Oriental\", \"Amber\", \"Incense\", \"Oud\", \"Patchouli\",       # 25-29: ì˜¤ë¦¬ì—”íƒˆ ê³„ì—´\n",
    "        \"Gourmand\", \"Vanilla\", \"Caramel\", \"Chocolate\", \"Coffee\",  # 30-34: êµ¬ë¥´ë§ ê³„ì—´\n",
    "        \"Musk\", \"Animalic\", \"Leather\", \"Tobacco\", \"Smoky\",        # 35-39: ê¹Šì€ ê³„ì—´\n",
    "        \"Powdery\", \"Clean\", \"Soap\", \"Marine\", \"Ozonic\"            # 40-44: íŒŒìš°ë”ë¦¬/í´ë¦° ê³„ì—´\n",
    "    ]\n",
    "    \n",
    "    return fragrance_accords[:45]  # ì •í™•íˆ 45ê°œë¡œ ë§ì¶¤def inspect_model_bundle(pkl_path: str):\n",
    "    \"\"\"ëª¨ë¸ ë²ˆë“¤ êµ¬ì¡°ë¥¼ ìì„¸íˆ ë¶„ì„í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ëª¨ë¸ ë²ˆë“¤ êµ¬ì¡° ë¶„ì„\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        obj = joblib.load(pkl_path)\n",
    "        print(f\"ë©”ì¸ ê°ì²´ íƒ€ì…: {type(obj)}\")\n",
    "        \n",
    "        if isinstance(obj, dict):\n",
    "            print(f\"ë”•ì…”ë„ˆë¦¬ í‚¤ ê°œìˆ˜: {len(obj)}\")\n",
    "            for key, value in obj.items():\n",
    "                print(f\"  '{key}': {type(value)}\")\n",
    "                \n",
    "                # ë¼ë²¨ ê´€ë ¨ ì†ì„± í™•ì¸\n",
    "                if hasattr(value, 'classes_'):\n",
    "                    print(f\"    â””â”€ classes_: {len(value.classes_)}ê°œ ({type(value.classes_[0]) if len(value.classes_) > 0 else 'empty'})\")\n",
    "                    if len(value.classes_) > 0:\n",
    "                        print(f\"       ìƒ˜í”Œ: {list(value.classes_[:3])}...\")\n",
    "                \n",
    "                # ë°°ì—´/ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ë‚´ìš© í™•ì¸\n",
    "                if isinstance(value, (list, tuple, np.ndarray)):\n",
    "                    print(f\"    â””â”€ ê¸¸ì´: {len(value)}\")\n",
    "                    if len(value) > 0:\n",
    "                        print(f\"       íƒ€ì…: {type(value[0])}\")\n",
    "                        if isinstance(value[0], str):\n",
    "                            print(f\"       ìƒ˜í”Œ: {list(value[:3])}...\")\n",
    "        \n",
    "        else:\n",
    "            print(\"ë‹¨ì¼ ê°ì²´ (ë¹„ë”•ì…”ë„ˆë¦¬)\")\n",
    "            if hasattr(obj, 'classes_'):\n",
    "                print(f\"  classes_: {len(obj.classes_)}ê°œ\")\n",
    "                print(f\"  ìƒ˜í”Œ: {list(obj.classes_[:3])}...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ë¶„ì„ ì‹¤íŒ¨: {e}\")\n",
    "    \n",
    "    print(\"=\"*60)# ============================================\n",
    "# model.pkl ë²ˆë“¤ ë¡œë“œ + MiniLM ì„ë² ë”© + ì˜ˆì¸¡\n",
    "# (joblib.load, ë§ˆìŠ¤í‚¹ í‰ê· í’€ë§, ë¼ë²¨/ì„ê³„ê°’ ì •ë ¬ í¬í•¨)\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "from typing import Any, Dict, List, Tuple\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# --------- ì„¤ì • ----------\n",
    "MODEL_PKL = os.getenv(\"MODEL_PKL\", \"./model.pkl\")  # ë²ˆë“¤ ê²½ë¡œ\n",
    "HF_MODEL  = os.getenv(\"HF_MODEL\", \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "MAX_LEN   = int(os.getenv(\"MAX_LEN\", 256))\n",
    "DEVICE    = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SZ  = int(os.getenv(\"BATCH_SZ\", 16))\n",
    "print(f\"[Device] {DEVICE}\")\n",
    "\n",
    "# --------- 1) ë²ˆë“¤ ë¡œë“œ (ìˆ˜ì •ë³¸) ----------\n",
    "def load_bundle(pkl_path: str) -> Dict[str, Any]:\n",
    "    print(f\"[DEBUG] ë²ˆë“¤ íŒŒì¼ ë¡œë”© ì¤‘: {pkl_path}\")\n",
    "    obj = joblib.load(pkl_path)  # âœ… joblib.loadë¡œ ì½ì–´ì•¼ í•¨\n",
    "    \n",
    "    # ë²ˆë“¤ êµ¬ì¡° ë””ë²„ê¹…\n",
    "    print(f\"[DEBUG] ë¡œë“œëœ ê°ì²´ íƒ€ì…: {type(obj)}\")\n",
    "    if isinstance(obj, dict):\n",
    "        print(f\"[DEBUG] ë”•ì…”ë„ˆë¦¬ í‚¤ë“¤: {list(obj.keys())}\")\n",
    "        bundle = obj\n",
    "    else:\n",
    "        print(f\"[DEBUG] ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹˜. ë¶„ë¥˜ê¸°ë¡œ ê°€ì •\")\n",
    "        bundle = {\"classifier\": obj}\n",
    "\n",
    "    # ë¶„ë¥˜ê¸° ì°¾ê¸°\n",
    "    clf = bundle.get(\"classifier\") or bundle.get(\"model\") or bundle.get(\"clf\")\n",
    "    if clf is None:\n",
    "        # ê°€ëŠ¥í•œ ëª¨ë“  í‚¤ í™•ì¸\n",
    "        for key, value in bundle.items():\n",
    "            if hasattr(value, \"predict_proba\"):\n",
    "                print(f\"[DEBUG] '{key}'ì—ì„œ predict_proba ë°œê²¬\")\n",
    "                clf = value\n",
    "                break\n",
    "    \n",
    "    if clf is None or not hasattr(clf, \"predict_proba\"):\n",
    "        raise ValueError(\"bundleì— predict_proba ê°€ëŠ¥í•œ ë¶„ë¥˜ê¸°ê°€ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    print(f\"[DEBUG] ë¶„ë¥˜ê¸° íƒ€ì…: {type(clf)}\")\n",
    "    \n",
    "    # MultiLabelBinarizer ì°¾ê¸°\n",
    "    mlb = bundle.get(\"mlb\") or bundle.get(\"multilabel_binarizer\") or bundle.get(\"label_binarizer\")\n",
    "    if mlb is None:\n",
    "        for key, value in bundle.items():\n",
    "            if hasattr(value, \"classes_\") and hasattr(value, \"transform\"):\n",
    "                print(f\"[DEBUG] '{key}'ì—ì„œ MultiLabelBinarizer ë°œê²¬\")\n",
    "                mlb = value\n",
    "                break\n",
    "    \n",
    "    if mlb:\n",
    "        print(f\"[DEBUG] MLB íƒ€ì…: {type(mlb)}, classes ê°œìˆ˜: {len(mlb.classes_) if hasattr(mlb, 'classes_') else 'None'}\")\n",
    "    \n",
    "    # ë¼ë²¨ ì°¾ê¸° (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)\n",
    "    labels = None\n",
    "    \n",
    "    # 1. ì§ì ‘ ì €ì¥ëœ labels\n",
    "    labels = bundle.get(\"labels\") or bundle.get(\"label_names\") or bundle.get(\"classes\") or bundle.get(\"class_names\")\n",
    "    \n",
    "    # 2. MLBì—ì„œ ë³µì›\n",
    "    if labels is None and mlb is not None and hasattr(mlb, \"classes_\"):\n",
    "        labels = list(mlb.classes_)\n",
    "        print(f\"[DEBUG] MLBì—ì„œ ë¼ë²¨ ë³µì›: {len(labels)}ê°œ\")\n",
    "        print(f\"[DEBUG] ë¼ë²¨ ìƒ˜í”Œ: {labels[:5] if len(labels) > 5 else labels}\")\n",
    "    \n",
    "    # 3. ë¶„ë¥˜ê¸°ì—ì„œ ë³µì›\n",
    "    if labels is None and hasattr(clf, \"classes_\"):\n",
    "        labels = list(clf.classes_)\n",
    "        print(f\"[DEBUG] ë¶„ë¥˜ê¸°ì—ì„œ ë¼ë²¨ ë³µì›: {len(labels)}ê°œ\")\n",
    "    \n",
    "    # 4. ë²ˆë“¤ì˜ ëª¨ë“  í‚¤ì—ì„œ ë¼ë²¨ ì°¾ê¸°\n",
    "    if labels is None:\n",
    "        for key, value in bundle.items():\n",
    "            if isinstance(value, (list, tuple, np.ndarray)) and len(value) > 0:\n",
    "                if isinstance(value[0], str):  # ë¬¸ìì—´ ë°°ì—´ì´ë©´ ë¼ë²¨ì¼ ê°€ëŠ¥ì„±\n",
    "                    print(f\"[DEBUG] '{key}'ì—ì„œ ë¬¸ìì—´ ë°°ì—´ ë°œê²¬: {len(value)}ê°œ\")\n",
    "                    labels = list(value)\n",
    "                    break\n",
    "    \n",
    "    thresholds = bundle.get(\"thresholds\", 0.5)\n",
    "    print(f\"[DEBUG] ì„ê³„ê°’ íƒ€ì…: {type(thresholds)}\")\n",
    "\n",
    "    # ë¼ë²¨ ì •ë ¬/ê²€ì¦\n",
    "    if hasattr(clf, \"classes_\") and labels is not None:\n",
    "        clf_labels = list(clf.classes_)\n",
    "        if list(labels) != clf_labels:\n",
    "            print(f\"[DEBUG] ë¼ë²¨ ìˆœì„œ ë¶ˆì¼ì¹˜. ë¶„ë¥˜ê¸° ìˆœì„œë¡œ ì¬ì •ë ¬\")\n",
    "            idx_map = {lbl: i for i, lbl in enumerate(labels)}\n",
    "            labels = [lbl for lbl in clf_labels if lbl in idx_map]\n",
    "\n",
    "        # thresholdsê°€ dictë¼ë©´ clf ìˆœì„œì— ë§ì¶° ë°°ì—´ë¡œ ë³€í™˜\n",
    "        if isinstance(thresholds, dict):\n",
    "            thresholds = np.array([float(thresholds.get(lbl, 0.5)) for lbl in labels], dtype=float)\n",
    "\n",
    "    print(f\"[DEBUG] ìµœì¢… ë¼ë²¨ ê°œìˆ˜: {len(labels) if labels else 0}\")\n",
    "    return {\"classifier\": clf, \"mlb\": mlb, \"labels\": labels, \"thresholds\": thresholds}\n",
    "\n",
    "# --------- 2) ì„ë² ë” ë¡œë“œ ----------\n",
    "def load_embedder():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(HF_MODEL)\n",
    "    base_model = AutoModel.from_pretrained(HF_MODEL).to(DEVICE).eval()\n",
    "    return tokenizer, base_model\n",
    "\n",
    "@torch.inference_mode()\n",
    "def encode_texts(texts: List[str], tokenizer, base_model, max_len: int = MAX_LEN, batch_size: int = BATCH_SZ) -> np.ndarray:\n",
    "    out = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        enc = tokenizer(\n",
    "            batch, padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\"\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        last = base_model(**enc).last_hidden_state        # (B, L, H)\n",
    "        mask = enc[\"attention_mask\"].unsqueeze(-1)        # (B, L, 1)\n",
    "        summed = (last * mask).sum(dim=1)                 \n",
    "        counts = mask.sum(dim=1).clamp(min=1)             \n",
    "        emb = summed / counts                             \n",
    "        out.append(emb.detach().cpu().numpy())\n",
    "    return np.vstack(out)\n",
    "\n",
    "# --------- 3) í™•ë¥  í‘œì¤€í™” ----------\n",
    "def _standardize_proba(proba) -> np.ndarray:\n",
    "    if isinstance(proba, list):\n",
    "        cols = []\n",
    "        for p in proba:\n",
    "            p = np.asarray(p)\n",
    "            if p.ndim == 2 and p.shape[1] > 1:\n",
    "                cols.append(p[:, 1])\n",
    "            else:\n",
    "                cols.append(p.ravel())\n",
    "        proba = np.column_stack(cols)\n",
    "    proba = np.asarray(proba)\n",
    "    if proba.ndim == 1:\n",
    "        proba = proba.reshape(1, -1)\n",
    "    return proba\n",
    "\n",
    "# --------- 4) ì„ê³„ê°’ ì ìš© (ìˆ˜ì •ë³¸) ----------\n",
    "def apply_thresholds(p: np.ndarray, labels: List[str], thresholds) -> List[Tuple[str, float]]:\n",
    "    n = len(p)\n",
    "    \n",
    "    # labels ê²€ì¦ ë° ì²˜ë¦¬\n",
    "    if labels is None or len(labels) == 0:\n",
    "        print(f\"[WARN] labelsê°€ ì—†ìŒ. ì¸ë±ìŠ¤ ë²ˆí˜¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤ (len(p)={n})\")\n",
    "        labels = [f\"class_{i}\" for i in range(n)]\n",
    "    elif len(labels) != n:\n",
    "        print(f\"[WARN] len(p)={n}, len(labels)={len(labels)} â†’ ê¸¸ì´ ë§ì¶¤\")\n",
    "        if len(labels) < n:\n",
    "            # labelsê°€ ë¶€ì¡±í•˜ë©´ ì¸ë±ìŠ¤ë¡œ ì±„ì›€\n",
    "            labels = list(labels) + [f\"class_{i}\" for i in range(len(labels), n)]\n",
    "        else:\n",
    "            # labelsê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ìë¦„\n",
    "            labels = labels[:n]\n",
    "\n",
    "    # thresholds ì²˜ë¦¬\n",
    "    if isinstance(thresholds, dict) and labels is not None:\n",
    "        thr_arr = np.array([float(thresholds.get(lbl, 0.5)) for lbl in labels], dtype=float)\n",
    "    elif hasattr(thresholds, \"__len__\") and not isinstance(thresholds, (str, bytes)):\n",
    "        thr_arr = np.array(thresholds, dtype=float)\n",
    "        if len(thr_arr) != n:\n",
    "            thr_arr = np.full(n, float(np.median(thresholds) if len(thresholds) else 0.5), dtype=float)\n",
    "    else:\n",
    "        thr_arr = np.full(n, float(thresholds), dtype=float)\n",
    "\n",
    "    # ì„ê³„ê°’ ì ìš©\n",
    "    idx = np.where(p >= thr_arr)[0]\n",
    "    \n",
    "    # ì•ˆì „í•œ ì¸ë±ìŠ¤ ì ‘ê·¼\n",
    "    names = []\n",
    "    vals = []\n",
    "    for i in idx:\n",
    "        if i < len(labels):\n",
    "            names.append(labels[i])\n",
    "            vals.append(float(p[i]))\n",
    "    \n",
    "    return sorted(list(zip(names, vals)), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# --------- 5) ì˜ˆì¸¡ í´ë˜ìŠ¤ ----------\n",
    "class TextClassifier:\n",
    "    def __init__(self, model_path: str):\n",
    "        print(f\"[Loading] Model bundle from: {model_path}\")\n",
    "        self.bundle = load_bundle(model_path)\n",
    "        self.clf = self.bundle[\"classifier\"]\n",
    "        self.labels = self.bundle[\"labels\"]\n",
    "        self.thresholds = self.bundle[\"thresholds\"]\n",
    "        \n",
    "        print(f\"[Loading] Embedder: {HF_MODEL}\")\n",
    "        self.tokenizer, self.base_model = load_embedder()\n",
    "        \n",
    "        print(f\"[Loaded] Labels: {len(self.labels) if self.labels else 0}\")\n",
    "        if self.labels and len(self.labels) > 0:\n",
    "            print(f\"[Loaded] Sample labels: {self.labels[:5]}{'...' if len(self.labels) > 5 else ''}\")\n",
    "        else:\n",
    "            print(f\"[WARN] labelsê°€ ì—†ìŒ - ëŸ°íƒ€ì„ì— class_0, class_1... í˜•íƒœë¡œ ìƒì„±ë¨\")\n",
    "        print(f\"[Loaded] Thresholds type: {type(self.thresholds).__name__}\")\n",
    "\n",
    "    def predict_labels(self, texts: List[str], topk: int = 3) -> List[Dict[str, Any]]:\n",
    "        vec = encode_texts(texts, self.tokenizer, self.base_model)\n",
    "        proba = _standardize_proba(self.clf.predict_proba(vec))\n",
    "        results = []\n",
    "        \n",
    "        for i, t in enumerate(texts):\n",
    "            p = proba[i]\n",
    "            n_classes = len(p)\n",
    "            \n",
    "            # labels ì•ˆì „ì„± ê²€ì‚¬\n",
    "            if self.labels is None or len(self.labels) == 0:\n",
    "                current_labels = [f\"class_{j}\" for j in range(n_classes)]\n",
    "                print(f\"[INFO] labels ì—†ìŒ â†’ class_0, class_1, ... ì‚¬ìš©\")\n",
    "            elif len(self.labels) != n_classes:\n",
    "                current_labels = list(self.labels) + [f\"class_{j}\" for j in range(len(self.labels), n_classes)]\n",
    "                if len(current_labels) > n_classes:\n",
    "                    current_labels = current_labels[:n_classes]\n",
    "            else:\n",
    "                current_labels = self.labels\n",
    "            \n",
    "            # TopK ê³„ì‚°\n",
    "            idx_sorted = np.argsort(p)[::-1][:topk]\n",
    "            top_pairs = [(current_labels[j], float(p[j])) for j in idx_sorted if j < len(current_labels)]\n",
    "            \n",
    "            # ì„ê³„ê°’ ì ìš©\n",
    "            passed = apply_thresholds(p, current_labels, self.thresholds)\n",
    "            \n",
    "            results.append({\n",
    "                \"text\": t,\n",
    "                \"topk\": top_pairs,\n",
    "                \"passed_threshold\": passed,\n",
    "                \"all_probs\": p,\n",
    "            })\n",
    "        return results\n",
    "\n",
    "    def predict_one(self, text: str, topk: int = 5) -> Dict[str, Any]:\n",
    "        out = self.predict_labels([text], topk=topk)[0]\n",
    "        print(f\"\\n[Query] {out['text']}\")\n",
    "        print(f\" - TopK : {', '.join(f'{n}({s:.3f})' for n, s in out['topk'])}\")\n",
    "        if out[\"passed_threshold\"]:\n",
    "            print(f\" - Pass : {', '.join(f'{n}({s:.3f})' for n, s in out['passed_threshold'])}\")\n",
    "        else:\n",
    "            print(\" - Pass : (ì„ê³„ê°’ í†µê³¼ ì—†ìŒ â†’ TopK ì°¸ê³ )\")\n",
    "        return out\n",
    "\n",
    "# ============================================\n",
    "# í…ŒìŠ¤íŠ¸ ì½”ë“œ\n",
    "# ============================================\n",
    "\n",
    "def run_tests():\n",
    "    \"\"\"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ í•¨ìˆ˜\"\"\"\n",
    "    \n",
    "    # ë¨¼ì € ëª¨ë¸ êµ¬ì¡° ë¶„ì„\n",
    "    inspect_model_bundle(MODEL_PKL)\n",
    "    \n",
    "    # í–¥ìˆ˜ ê´€ë ¨ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë“¤ (ë©”ì¸ ì–´ì½”ë“œ ì¶”ì¶œìš©)\n",
    "    test_queries = [\n",
    "        # ê³„ì ˆë³„ í–¥ìˆ˜ ì¶”ì²œ\n",
    "        \"ì—¬ë¦„ì— ë¿Œë¦´ë§Œí•œ í–¥ìˆ˜ ì¶”ì²œí•´ì£¼ì„¸ìš”\",\n",
    "        \"ê²¨ìš¸ì— ì–´ìš¸ë¦¬ëŠ” ë”°ëœ»í•œ í–¥ìˆ˜ê°€ ë­ê°€ ìˆì„ê¹Œìš”?\",\n",
    "        \"ë´„ì— ì–´ìš¸ë¦¬ëŠ” í”Œë¡œëŸ´ í–¥ìˆ˜ ì¶”ì²œí•´ì£¼ì„¸ìš”\",\n",
    "        \"ê°€ì„ì— ë¿Œë¦¬ê¸° ì¢‹ì€ ìš°ë””í•œ í–¥ìˆ˜ëŠ”?\",\n",
    "        \n",
    "        # ìƒí™©ë³„ í–¥ìˆ˜ ì¶”ì²œ\n",
    "        \"ë°ì´íŠ¸í•  ë•Œ ë¿Œë¦¬ë©´ ì¢‹ì€ í–¥ìˆ˜ ì¶”ì²œ\",\n",
    "        \"ì§ì¥ì—ì„œ ë¿Œë ¤ë„ ë˜ëŠ” ì€ì€í•œ í–¥ìˆ˜ëŠ”?\",\n",
    "        \"íŒŒí‹°ë‚˜ í´ëŸ½ì— ì–´ìš¸ë¦¬ëŠ” ì„¹ì‹œí•œ í–¥ìˆ˜ ì¶”ì²œ\",\n",
    "        \"ìš´ë™í•  ë•Œ ë¿Œë ¤ë„ ì¢‹ì€ ìƒì¾Œí•œ í–¥ìˆ˜ëŠ”?\",\n",
    "        \n",
    "        # ì„±ë³„/ì—°ë ¹ë³„ ì¶”ì²œ\n",
    "        \"20ëŒ€ ì—¬ì„±ì—ê²Œ ì–´ìš¸ë¦¬ëŠ” í–¥ìˆ˜ ì¶”ì²œí•´ì£¼ì„¸ìš”\",\n",
    "        \"30ëŒ€ ë‚¨ì„±ì´ ì“°ê¸° ì¢‹ì€ í–¥ìˆ˜ëŠ”?\",\n",
    "        \"10ëŒ€ê°€ ì“°ê¸°ì— ë¶€ë‹´ì—†ëŠ” í–¥ìˆ˜ ì¶”ì²œ\",\n",
    "        \"50ëŒ€ ì—¬ì„±ì—ê²Œ ì–´ìš¸ë¦¬ëŠ” ìš°ì•„í•œ í–¥ìˆ˜ëŠ”?\",\n",
    "        \n",
    "        # í–¥ì¡°/ì–´ì½”ë“œë³„ ë¬¸ì˜\n",
    "        \"ì‹œíŠ¸ëŸ¬ìŠ¤ ê³„ì—´ì˜ ìƒí¼í•œ í–¥ìˆ˜ ì¶”ì²œ\",\n",
    "        \"ë°”ë‹ë¼ í–¥ì´ ê°•í•œ ë‹¬ì½¤í•œ í–¥ìˆ˜ëŠ”?\",\n",
    "        \"ë¨¸ìŠ¤í¬ í–¥ì´ ë“¤ì–´ê°„ í–¥ìˆ˜ ì¶”ì²œí•´ì£¼ì„¸ìš”\",\n",
    "        \"ìš°ë”” ê³„ì—´ì˜ ì¤‘í›„í•œ í–¥ìˆ˜ê°€ ë­ê°€ ìˆë‚˜ìš”?\",\n",
    "        \"í”Œë¡œëŸ´ ë¶€ì¼€ í–¥ìˆ˜ ì¤‘ì— ì¶”ì²œí•´ì£¼ì„¸ìš”\",\n",
    "        \n",
    "        # ë¸Œëœë“œ/ê°€ê²©ëŒ€ë³„\n",
    "        \"ìƒ¤ë„¬ í–¥ìˆ˜ ì¤‘ì— ì¸ê¸°ìˆëŠ”ê±° ì¶”ì²œ\",\n",
    "        \"10ë§Œì› ì´í•˜ë¡œ ì‚´ ìˆ˜ ìˆëŠ” ì¢‹ì€ í–¥ìˆ˜ëŠ”?\",\n",
    "        \"ë””ì˜¬ í–¥ìˆ˜ ì¤‘ì— ì—¬ë¦„ìš©ìœ¼ë¡œ ì¢‹ì€ê±´?\",\n",
    "        \"í•™ìƒë„ ì‚´ ìˆ˜ ìˆëŠ” ì €ë ´í•œ í–¥ìˆ˜ ì¶”ì²œ\",\n",
    "        \n",
    "        # íŠ¹ì • í–¥ ì„ í˜¸ë„\n",
    "        \"ì¥ë¯¸í–¥ì´ ë‚˜ëŠ” í–¥ìˆ˜ ì¶”ì²œí•´ì£¼ì„¸ìš”\",\n",
    "        \"ì˜¤ì…˜ ëƒ„ìƒˆê°€ ë‚˜ëŠ” ì‹œì›í•œ í–¥ìˆ˜ëŠ”?\",\n",
    "        \"ê³¼ì¼í–¥ì´ ê°•í•œ í”„ë£¨í‹°í•œ í–¥ìˆ˜ ì¶”ì²œ\",\n",
    "        \"ì»¤í”¼ë‚˜ ì´ˆì½œë¦¿ í–¥ì´ ë‚˜ëŠ” í–¥ìˆ˜ëŠ”?\",\n",
    "        \"í—ˆë¸Œí–¥ì´ ë“¤ì–´ê°„ ìì—°ìŠ¤ëŸ¬ìš´ í–¥ìˆ˜ ì¶”ì²œ\"\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # ë¶„ë¥˜ê¸° ì´ˆê¸°í™”\n",
    "        print(f\"\\nëª¨ë¸ ë¡œë”©ì„ ì‹œë„í•©ë‹ˆë‹¤...\")\n",
    "        classifier = TextClassifier(MODEL_PKL)\n",
    "        \n",
    "        # ë¼ë²¨ì´ ì—†ìœ¼ë©´ ìˆ˜ë™ìœ¼ë¡œ ìƒì„± ì œì•ˆ\n",
    "        if not classifier.labels or len(classifier.labels) == 0:\n",
    "            print(f\"\\nğŸ’¡ ë¼ë²¨ì´ ì—†ìœ¼ë¯€ë¡œ í–¥ìˆ˜ ì–´ì½”ë“œë¥¼ ìˆ˜ë™ìœ¼ë¡œ ë§¤í•‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "            manual_labels = create_manual_labels()\n",
    "            \n",
    "            response = input(f\"ìˆ˜ë™ í–¥ìˆ˜ ì–´ì½”ë“œ ë¼ë²¨ì„ ì‚¬ìš©í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): \").strip().lower()\n",
    "            if response in ['y', 'yes', 'ë„¤', 'ã…‡']:\n",
    "                classifier.labels = manual_labels\n",
    "                print(f\"âœ… í–¥ìˆ˜ ì–´ì½”ë“œ ë¼ë²¨ {len(manual_labels)}ê°œë¥¼ ì ìš©í–ˆìŠµë‹ˆë‹¤.\")\n",
    "                print(f\"ìƒ˜í”Œ: {manual_labels[:5]}...\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"í–¥ìˆ˜ ë©”ì¸ ì–´ì½”ë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ (predict_one)\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # ê°œë³„ í…ŒìŠ¤íŠ¸ (ìƒìœ„ 3ê°œ)\n",
    "        for i, query in enumerate(test_queries[:5]):  # ì²˜ìŒ 5ê°œë§Œ\n",
    "            print(f\"\\n[Test {i+1}]\")\n",
    "            result = classifier.predict_one(query, topk=3)\n",
    "            \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ë°°ì¹˜ í–¥ìˆ˜ ì–´ì½”ë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # ë°°ì¹˜ í…ŒìŠ¤íŠ¸\n",
    "        batch_results = classifier.predict_labels(test_queries[:8], topk=3)\n",
    "        \n",
    "        for i, result in enumerate(batch_results):\n",
    "            print(f\"\\n[Batch {i+1}] {result['text'][:50]}...\")\n",
    "            print(f\"  Top3: {', '.join(f'{n}({s:.3f})' for n, s in result['topk'])}\")\n",
    "            if result['passed_threshold']:\n",
    "                print(f\"  Pass: {', '.join(f'{n}({s:.3f})' for n, s in result['passed_threshold'])}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ë©”ì¸ ì–´ì½”ë“œ ì¶”ì¶œ í†µê³„\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # ì„ê³„ê°’ í†µê³¼ í†µê³„\n",
    "        total_tests = len(batch_results)\n",
    "        passed_tests = sum(1 for r in batch_results if r['passed_threshold'])\n",
    "        \n",
    "        print(f\"ì „ì²´ í…ŒìŠ¤íŠ¸: {total_tests}\")\n",
    "        print(f\"ì„ê³„ê°’ í†µê³¼: {passed_tests}\")\n",
    "        print(f\"ì–´ì½”ë“œ ì¶”ì¶œìœ¨: {passed_tests/total_tests*100:.1f}%\")\n",
    "        \n",
    "        # ì–´ì½”ë“œë³„ í†µê³„\n",
    "        accord_counts = {}\n",
    "        for result in batch_results:\n",
    "            for accord, score in result['passed_threshold']:\n",
    "                accord_counts[accord] = accord_counts.get(accord, 0) + 1\n",
    "        \n",
    "        if accord_counts:\n",
    "            print(\"\\në©”ì¸ ì–´ì½”ë“œë³„ ë“±ì¥ íšŸìˆ˜:\")\n",
    "            for accord, count in sorted(accord_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "                print(f\"  {accord}: {count}íšŒ\")\n",
    "        \n",
    "        print(\"\\ní…ŒìŠ¤íŠ¸ ì™„ë£Œ!\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"[ERROR] ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {MODEL_PKL}\")\n",
    "        print(\"ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”:\")\n",
    "        print(\"1. model.pkl íŒŒì¼ì´ í˜„ì¬ ë””ë ‰í† ë¦¬ì— ìˆëŠ”ì§€ í™•ì¸\")\n",
    "        print(\"2. MODEL_PKL í™˜ê²½ë³€ìˆ˜ë¡œ ì˜¬ë°”ë¥¸ ê²½ë¡œ ì„¤ì •\")\n",
    "        print(\"3. ì˜ˆì‹œ: export MODEL_PKL='/path/to/your/model.pkl'\")\n",
    "        \n",
    "    except ValueError as e:\n",
    "        print(f\"[ERROR] ëª¨ë¸ ë¡œë”© ì˜¤ë¥˜: {str(e)}\")\n",
    "        print(\"model.pkl íŒŒì¼ì˜ êµ¬ì¡°ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\")\n",
    "        print(f\"ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "def interactive_test():\n",
    "    \"\"\"ëŒ€í™”í˜• í…ŒìŠ¤íŠ¸ í•¨ìˆ˜\"\"\"\n",
    "    try:\n",
    "        classifier = TextClassifier(MODEL_PKL)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"í–¥ìˆ˜ ë©”ì¸ ì–´ì½”ë“œ ì¶”ì¶œ - ëŒ€í™”í˜• ëª¨ë“œ\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"í–¥ìˆ˜ ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ ë©”ì¸ ì–´ì½”ë“œ 3ê°œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.\")\n",
    "        print(\"ì˜ˆì‹œ: 'ì—¬ë¦„ì— ë¿Œë¦´ë§Œí•œ í–¥ìˆ˜ ì¶”ì²œ', 'ë°ì´íŠ¸ìš© í–¥ìˆ˜ ì¶”ì²œ' ë“±\")\n",
    "        print(\"ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ë˜ëŠ” 'exit'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.\")\n",
    "        \n",
    "        while True:\n",
    "            query = input(\"\\ní–¥ìˆ˜ ì§ˆë¬¸ ì…ë ¥: \").strip()\n",
    "            if query.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:\n",
    "                break\n",
    "            if not query:\n",
    "                continue\n",
    "                \n",
    "            result = classifier.predict_one(query, topk=3)  # ë©”ì¸ ì–´ì½”ë“œ 3ê°œ\n",
    "            \n",
    "            # ì¶”ê°€ ì •ë³´ í‘œì‹œ\n",
    "            if result[\"passed_threshold\"]:\n",
    "                print(f\"\\nâœ… ì¶”ì¶œëœ ë©”ì¸ ì–´ì½”ë“œ: {len(result['passed_threshold'])}ê°œ\")\n",
    "            else:\n",
    "                print(f\"\\nâš ï¸  ì„ê³„ê°’ì„ ë„˜ëŠ” ì–´ì½”ë“œê°€ ì—†ì–´ ìƒìœ„ 3ê°œë¥¼ ì°¸ê³ í•˜ì„¸ìš”\")\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\ní”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {str(e)}\")\n",
    "\n",
    "# ============================================\n",
    "# ë©”ì¸ ì‹¤í–‰ë¶€\n",
    "# ============================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"í–¥ìˆ˜ ë©”ì¸ ì–´ì½”ë“œ ì¶”ì¶œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # í™˜ê²½ ì •ë³´ ì¶œë ¥\n",
    "    print(f\"Model Path: {MODEL_PKL}\")\n",
    "    print(f\"HF Model: {HF_MODEL}\")\n",
    "    print(f\"Device: {DEVICE}\")\n",
    "    print(f\"Max Length: {MAX_LEN}\")\n",
    "    print(f\"Batch Size: {BATCH_SZ}\")\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ì„ íƒ\n",
    "    import sys\n",
    "    if len(sys.argv) > 1 and sys.argv[1] == \"interactive\":\n",
    "        interactive_test()\n",
    "    else:\n",
    "        run_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23cb9f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Device] cpu\n",
      "í–¥ìˆ˜ ë©”ì¸ ì–´ì½”ë“œ ì¶”ì¶œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸\n",
      "============================================================\n",
      "Model Path: ./model.pkl\n",
      "HF Model: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "Device: cpu\n",
      "Max Length: 256\n",
      "Batch Size: 16\n",
      "\n",
      "============================================================\n",
      "ëª¨ë¸ ë²ˆë“¤ êµ¬ì¡° ë¶„ì„\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [15:07:14] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\gbm\\gbtree.cc:359: \n",
      "  Loading from a raw memory buffer (like pickle in Python, RDS in R) on a CPU-only\n",
      "  machine. Consider using `save_model/load_model` instead. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "\n",
      "  for more details about differences between saving model and serializing.  Changing `tree_method` to `hist`.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [15:07:14] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\gbm\\gbtree.cc:384: Changing updater from `grow_gpu_hist` to `grow_quantile_histmaker`.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [15:07:14] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  setstate(state)\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\pickle.py:1760: UserWarning: [15:07:14] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  setstate(state)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë©”ì¸ ê°ì²´ íƒ€ì…: <class 'dict'>\n",
      "ë”•ì…”ë„ˆë¦¬ í‚¤ ê°œìˆ˜: 3\n",
      "  'classifier': <class 'sklearn.multiclass.OneVsRestClassifier'>\n",
      "    â””â”€ classes_: 45ê°œ (<class 'numpy.int64'>)\n",
      "       ìƒ˜í”Œ: [np.int64(0), np.int64(1), np.int64(2)]...\n",
      "  'mlb': <class 'sklearn.preprocessing._label.MultiLabelBinarizer'>\n",
      "    â””â”€ classes_: 45ê°œ (<class 'str'>)\n",
      "       ìƒ˜í”Œ: ['$$$', 'Amber', 'Aromatic']...\n",
      "  'thresholds': <class 'dict'>\n",
      "============================================================\n",
      "\n",
      "ëª¨ë¸ ë¡œë”©ì„ ì‹œë„í•©ë‹ˆë‹¤...\n",
      "[Loading] Model bundle from: ./model.pkl\n",
      "[DEBUG] ë²ˆë“¤ íŒŒì¼ ë¡œë”© ì¤‘: ./model.pkl\n",
      "[DEBUG] ë¡œë“œëœ ê°ì²´ íƒ€ì…: <class 'dict'>\n",
      "[DEBUG] ë”•ì…”ë„ˆë¦¬ í‚¤ë“¤: ['classifier', 'mlb', 'thresholds']\n",
      "[DEBUG] ë¶„ë¥˜ê¸° íƒ€ì…: <class 'sklearn.multiclass.OneVsRestClassifier'>\n",
      "[DEBUG] MLB íƒ€ì…: <class 'sklearn.preprocessing._label.MultiLabelBinarizer'>, classes ê°œìˆ˜: 45\n",
      "[DEBUG] MLBì—ì„œ ë¼ë²¨ ë³µì›: 45ê°œ\n",
      "[DEBUG] ë¼ë²¨ ìƒ˜í”Œ: ['$$$', 'Amber', 'Aromatic', 'Blossom', 'Bouquet']\n",
      "[DEBUG] ì„ê³„ê°’ íƒ€ì…: <class 'dict'>\n",
      "[DEBUG] ë¼ë²¨ ìˆœì„œ ë¶ˆì¼ì¹˜. ë¶„ë¥˜ê¸° ìˆœì„œë¡œ ì¬ì •ë ¬\n",
      "[DEBUG] ìµœì¢… ë¼ë²¨ ê°œìˆ˜: 0\n",
      "[Loading] Embedder: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "[Loaded] Labels: 0\n",
      "[WARN] labelsê°€ ì—†ìŒ - ëŸ°íƒ€ì„ì— class_0, class_1... í˜•íƒœë¡œ ìƒì„±ë¨\n",
      "[Loaded] Thresholds type: ndarray\n",
      "\n",
      "ğŸ’¡ ë¼ë²¨ì´ ì—†ìœ¼ë¯€ë¡œ í–¥ìˆ˜ ì–´ì½”ë“œë¥¼ ìˆ˜ë™ìœ¼ë¡œ ë§¤í•‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "âœ… í–¥ìˆ˜ ì–´ì½”ë“œ ë¼ë²¨ 45ê°œë¥¼ ì ìš©í–ˆìŠµë‹ˆë‹¤.\n",
      "ìƒ˜í”Œ: ['Citrus', 'Fresh', 'Aquatic', 'Green', 'Herbal']...\n",
      "\n",
      "============================================================\n",
      "í–¥ìˆ˜ ë©”ì¸ ì–´ì½”ë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ (predict_one)\n",
      "============================================================\n",
      "\n",
      "[Test 1]\n",
      "\n",
      "[Query] ì—¬ë¦„ì— ë¿Œë¦´ë§Œí•œ í–¥ìˆ˜ ì¶”ì²œí•´ì£¼ì„¸ìš”\n",
      " - TopK : Berry(0.737), Rose(0.347), Spicy(0.321)\n",
      " - Pass : Berry(0.737)\n",
      "\n",
      "[Test 2]\n",
      "\n",
      "[Query] ê²¨ìš¸ì— ì–´ìš¸ë¦¬ëŠ” ë”°ëœ»í•œ í–¥ìˆ˜ê°€ ë­ê°€ ìˆì„ê¹Œìš”?\n",
      " - TopK : Berry(0.798), Powdery(0.643), Fresh(0.608)\n",
      " - Pass : Berry(0.798), Powdery(0.643), Fresh(0.608), Chocolate(0.584)\n",
      "\n",
      "[Test 3]\n",
      "\n",
      "[Query] ë´„ì— ì–´ìš¸ë¦¬ëŠ” í”Œë¡œëŸ´ í–¥ìˆ˜ ì¶”ì²œí•´ì£¼ì„¸ìš”\n",
      " - TopK : Berry(0.804), Fruity(0.444), Rose(0.303)\n",
      " - Pass : Berry(0.804)\n",
      "\n",
      "[Test 4]\n",
      "\n",
      "[Query] ê°€ì„ì— ë¿Œë¦¬ê¸° ì¢‹ì€ ìš°ë””í•œ í–¥ìˆ˜ëŠ”?\n",
      " - TopK : Berry(0.707), Rose(0.403), Powdery(0.309)\n",
      " - Pass : Berry(0.707)\n",
      "\n",
      "[Test 5]\n",
      "\n",
      "[Query] ë°ì´íŠ¸í•  ë•Œ ë¿Œë¦¬ë©´ ì¢‹ì€ í–¥ìˆ˜ ì¶”ì²œ\n",
      " - TopK : Berry(0.915), Spicy(0.473), Powdery(0.384)\n",
      " - Pass : Berry(0.915)\n",
      "\n",
      "============================================================\n",
      "ë°°ì¹˜ í–¥ìˆ˜ ì–´ì½”ë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸\n",
      "============================================================\n",
      "\n",
      "[Batch 1] ì—¬ë¦„ì— ë¿Œë¦´ë§Œí•œ í–¥ìˆ˜ ì¶”ì²œí•´ì£¼ì„¸ìš”...\n",
      "  Top3: Berry(0.737), Rose(0.347), Spicy(0.321)\n",
      "  Pass: Berry(0.737)\n",
      "\n",
      "[Batch 2] ê²¨ìš¸ì— ì–´ìš¸ë¦¬ëŠ” ë”°ëœ»í•œ í–¥ìˆ˜ê°€ ë­ê°€ ìˆì„ê¹Œìš”?...\n",
      "  Top3: Berry(0.798), Powdery(0.643), Fresh(0.608)\n",
      "  Pass: Berry(0.798), Powdery(0.643), Fresh(0.608), Chocolate(0.584)\n",
      "\n",
      "[Batch 3] ë´„ì— ì–´ìš¸ë¦¬ëŠ” í”Œë¡œëŸ´ í–¥ìˆ˜ ì¶”ì²œí•´ì£¼ì„¸ìš”...\n",
      "  Top3: Berry(0.804), Fruity(0.444), Rose(0.303)\n",
      "  Pass: Berry(0.804)\n",
      "\n",
      "[Batch 4] ê°€ì„ì— ë¿Œë¦¬ê¸° ì¢‹ì€ ìš°ë””í•œ í–¥ìˆ˜ëŠ”?...\n",
      "  Top3: Berry(0.707), Rose(0.403), Powdery(0.309)\n",
      "  Pass: Berry(0.707)\n",
      "\n",
      "[Batch 5] ë°ì´íŠ¸í•  ë•Œ ë¿Œë¦¬ë©´ ì¢‹ì€ í–¥ìˆ˜ ì¶”ì²œ...\n",
      "  Top3: Berry(0.915), Spicy(0.473), Powdery(0.384)\n",
      "  Pass: Berry(0.915)\n",
      "\n",
      "[Batch 6] ì§ì¥ì—ì„œ ë¿Œë ¤ë„ ë˜ëŠ” ì€ì€í•œ í–¥ìˆ˜ëŠ”?...\n",
      "  Top3: Berry(0.785), Fresh(0.346), Rose(0.280)\n",
      "  Pass: Berry(0.785)\n",
      "\n",
      "[Batch 7] íŒŒí‹°ë‚˜ í´ëŸ½ì— ì–´ìš¸ë¦¬ëŠ” ì„¹ì‹œí•œ í–¥ìˆ˜ ì¶”ì²œ...\n",
      "  Top3: Berry(0.805), Fresh(0.609), Spicy(0.474)\n",
      "  Pass: Berry(0.805), Fresh(0.609)\n",
      "\n",
      "[Batch 8] ìš´ë™í•  ë•Œ ë¿Œë ¤ë„ ì¢‹ì€ ìƒì¾Œí•œ í–¥ìˆ˜ëŠ”?...\n",
      "  Top3: Berry(0.864), Rose(0.395), Pear(0.161)\n",
      "  Pass: Berry(0.864)\n",
      "\n",
      "============================================================\n",
      "ë©”ì¸ ì–´ì½”ë“œ ì¶”ì¶œ í†µê³„\n",
      "============================================================\n",
      "ì „ì²´ í…ŒìŠ¤íŠ¸: 8\n",
      "ì„ê³„ê°’ í†µê³¼: 8\n",
      "ì–´ì½”ë“œ ì¶”ì¶œìœ¨: 100.0%\n",
      "\n",
      "ë©”ì¸ ì–´ì½”ë“œë³„ ë“±ì¥ íšŸìˆ˜:\n",
      "  Berry: 8íšŒ\n",
      "  Fresh: 2íšŒ\n",
      "  Powdery: 1íšŒ\n",
      "  Chocolate: 1íšŒ\n",
      "\n",
      "í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "def create_manual_labels():\n",
    "    \"\"\"ìˆ˜ë™ìœ¼ë¡œ í–¥ìˆ˜ ì–´ì½”ë“œ ë¼ë²¨ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ (ì°¸ê³ ìš©)\"\"\"\n",
    "    \n",
    "    # ì¼ë°˜ì ì¸ í–¥ìˆ˜ ì–´ì½”ë“œë“¤ (45ê°œ í´ë˜ìŠ¤ ì˜ˆì‹œ)\n",
    "    fragrance_accords = [\n",
    "        \"Citrus\", \"Fresh\", \"Aquatic\", \"Green\", \"Herbal\",           # 0-4: ìƒì¾Œí•œ ê³„ì—´\n",
    "        \"Floral\", \"Rose\", \"Jasmine\", \"Lily\", \"Peony\",             # 5-9: í”Œë¡œëŸ´ ê³„ì—´  \n",
    "        \"Fruity\", \"Apple\", \"Pear\", \"Peach\", \"Berry\",              # 10-14: ê³¼ì¼ ê³„ì—´\n",
    "        \"Spicy\", \"Pepper\", \"Cinnamon\", \"Ginger\", \"Cardamom\",      # 15-19: ìŠ¤íŒŒì´ì‹œ ê³„ì—´\n",
    "        \"Woody\", \"Sandalwood\", \"Cedar\", \"Pine\", \"Vetiver\",        # 20-24: ìš°ë”” ê³„ì—´\n",
    "        \"Oriental\", \"Amber\", \"Incense\", \"Oud\", \"Patchouli\",       # 25-29: ì˜¤ë¦¬ì—”íƒˆ ê³„ì—´\n",
    "        \"Gourmand\", \"Vanilla\", \"Caramel\", \"Chocolate\", \"Coffee\",  # 30-34: êµ¬ë¥´ë§ ê³„ì—´\n",
    "        \"Musk\", \"Animalic\", \"Leather\", \"Tobacco\", \"Smoky\",        # 35-39: ê¹Šì€ ê³„ì—´\n",
    "        \"Powdery\", \"Clean\", \"Soap\", \"Marine\", \"Ozonic\"            # 40-44: íŒŒìš°ë”ë¦¬/í´ë¦° ê³„ì—´\n",
    "    ]\n",
    "    \n",
    "    return fragrance_accords[:45]  # ì •í™•íˆ 45ê°œë¡œ ë§ì¶¤def inspect_model_bundle(pkl_path: str):\n",
    "    \"\"\"ëª¨ë¸ ë²ˆë“¤ êµ¬ì¡°ë¥¼ ìì„¸íˆ ë¶„ì„í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ëª¨ë¸ ë²ˆë“¤ êµ¬ì¡° ë¶„ì„\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        obj = joblib.load(pkl_path)\n",
    "        print(f\"ë©”ì¸ ê°ì²´ íƒ€ì…: {type(obj)}\")\n",
    "        \n",
    "        if isinstance(obj, dict):\n",
    "            print(f\"ë”•ì…”ë„ˆë¦¬ í‚¤ ê°œìˆ˜: {len(obj)}\")\n",
    "            for key, value in obj.items():\n",
    "                print(f\"  '{key}': {type(value)}\")\n",
    "                \n",
    "                # ë¼ë²¨ ê´€ë ¨ ì†ì„± í™•ì¸\n",
    "                if hasattr(value, 'classes_'):\n",
    "                    print(f\"    â””â”€ classes_: {len(value.classes_)}ê°œ ({type(value.classes_[0]) if len(value.classes_) > 0 else 'empty'})\")\n",
    "                    if len(value.classes_) > 0:\n",
    "                        print(f\"       ìƒ˜í”Œ: {list(value.classes_[:3])}...\")\n",
    "                \n",
    "                # ë°°ì—´/ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ë‚´ìš© í™•ì¸\n",
    "                if isinstance(value, (list, tuple, np.ndarray)):\n",
    "                    print(f\"    â””â”€ ê¸¸ì´: {len(value)}\")\n",
    "                    if len(value) > 0:\n",
    "                        print(f\"       íƒ€ì…: {type(value[0])}\")\n",
    "                        if isinstance(value[0], str):\n",
    "                            print(f\"       ìƒ˜í”Œ: {list(value[:3])}...\")\n",
    "        \n",
    "        else:\n",
    "            print(\"ë‹¨ì¼ ê°ì²´ (ë¹„ë”•ì…”ë„ˆë¦¬)\")\n",
    "            if hasattr(obj, 'classes_'):\n",
    "                print(f\"  classes_: {len(obj.classes_)}ê°œ\")\n",
    "                print(f\"  ìƒ˜í”Œ: {list(obj.classes_[:3])}...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ë¶„ì„ ì‹¤íŒ¨: {e}\")\n",
    "    \n",
    "    print(\"=\"*60)# ============================================\n",
    "# model.pkl ë²ˆë“¤ ë¡œë“œ + MiniLM ì„ë² ë”© + ì˜ˆì¸¡\n",
    "# (joblib.load, ë§ˆìŠ¤í‚¹ í‰ê· í’€ë§, ë¼ë²¨/ì„ê³„ê°’ ì •ë ¬ í¬í•¨)\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "from typing import Any, Dict, List, Tuple\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# --------- ì„¤ì • ----------\n",
    "MODEL_PKL = os.getenv(\"MODEL_PKL\", \"./model.pkl\")  # ë²ˆë“¤ ê²½ë¡œ\n",
    "HF_MODEL  = os.getenv(\"HF_MODEL\", \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "MAX_LEN   = int(os.getenv(\"MAX_LEN\", 256))\n",
    "DEVICE    = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SZ  = int(os.getenv(\"BATCH_SZ\", 16))\n",
    "print(f\"[Device] {DEVICE}\")\n",
    "\n",
    "# --------- 1) ë²ˆë“¤ ë¡œë“œ (ìˆ˜ì •ë³¸) ----------\n",
    "def load_bundle(pkl_path: str) -> Dict[str, Any]:\n",
    "    print(f\"[DEBUG] ë²ˆë“¤ íŒŒì¼ ë¡œë”© ì¤‘: {pkl_path}\")\n",
    "    obj = joblib.load(pkl_path)  # âœ… joblib.loadë¡œ ì½ì–´ì•¼ í•¨\n",
    "    \n",
    "    # ë²ˆë“¤ êµ¬ì¡° ë””ë²„ê¹…\n",
    "    print(f\"[DEBUG] ë¡œë“œëœ ê°ì²´ íƒ€ì…: {type(obj)}\")\n",
    "    if isinstance(obj, dict):\n",
    "        print(f\"[DEBUG] ë”•ì…”ë„ˆë¦¬ í‚¤ë“¤: {list(obj.keys())}\")\n",
    "        bundle = obj\n",
    "    else:\n",
    "        print(f\"[DEBUG] ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹˜. ë¶„ë¥˜ê¸°ë¡œ ê°€ì •\")\n",
    "        bundle = {\"classifier\": obj}\n",
    "\n",
    "    # ë¶„ë¥˜ê¸° ì°¾ê¸°\n",
    "    clf = bundle.get(\"classifier\") or bundle.get(\"model\") or bundle.get(\"clf\")\n",
    "    if clf is None:\n",
    "        # ê°€ëŠ¥í•œ ëª¨ë“  í‚¤ í™•ì¸\n",
    "        for key, value in bundle.items():\n",
    "            if hasattr(value, \"predict_proba\"):\n",
    "                print(f\"[DEBUG] '{key}'ì—ì„œ predict_proba ë°œê²¬\")\n",
    "                clf = value\n",
    "                break\n",
    "    \n",
    "    if clf is None or not hasattr(clf, \"predict_proba\"):\n",
    "        raise ValueError(\"bundleì— predict_proba ê°€ëŠ¥í•œ ë¶„ë¥˜ê¸°ê°€ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    print(f\"[DEBUG] ë¶„ë¥˜ê¸° íƒ€ì…: {type(clf)}\")\n",
    "    \n",
    "    # MultiLabelBinarizer ì°¾ê¸°\n",
    "    mlb = bundle.get(\"mlb\") or bundle.get(\"multilabel_binarizer\") or bundle.get(\"label_binarizer\")\n",
    "    if mlb is None:\n",
    "        for key, value in bundle.items():\n",
    "            if hasattr(value, \"classes_\") and hasattr(value, \"transform\"):\n",
    "                print(f\"[DEBUG] '{key}'ì—ì„œ MultiLabelBinarizer ë°œê²¬\")\n",
    "                mlb = value\n",
    "                break\n",
    "    \n",
    "    if mlb:\n",
    "        print(f\"[DEBUG] MLB íƒ€ì…: {type(mlb)}, classes ê°œìˆ˜: {len(mlb.classes_) if hasattr(mlb, 'classes_') else 'None'}\")\n",
    "    \n",
    "    # ë¼ë²¨ ì°¾ê¸° (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)\n",
    "    labels = None\n",
    "    \n",
    "    # 1. ì§ì ‘ ì €ì¥ëœ labels\n",
    "    labels = bundle.get(\"labels\") or bundle.get(\"label_names\") or bundle.get(\"classes\") or bundle.get(\"class_names\")\n",
    "    \n",
    "    # 2. MLBì—ì„œ ë³µì›\n",
    "    if labels is None and mlb is not None and hasattr(mlb, \"classes_\"):\n",
    "        labels = list(mlb.classes_)\n",
    "        print(f\"[DEBUG] MLBì—ì„œ ë¼ë²¨ ë³µì›: {len(labels)}ê°œ\")\n",
    "        print(f\"[DEBUG] ë¼ë²¨ ìƒ˜í”Œ: {labels[:5] if len(labels) > 5 else labels}\")\n",
    "    \n",
    "    # 3. ë¶„ë¥˜ê¸°ì—ì„œ ë³µì›\n",
    "    if labels is None and hasattr(clf, \"classes_\"):\n",
    "        labels = list(clf.classes_)\n",
    "        print(f\"[DEBUG] ë¶„ë¥˜ê¸°ì—ì„œ ë¼ë²¨ ë³µì›: {len(labels)}ê°œ\")\n",
    "    \n",
    "    # 4. ë²ˆë“¤ì˜ ëª¨ë“  í‚¤ì—ì„œ ë¼ë²¨ ì°¾ê¸°\n",
    "    if labels is None:\n",
    "        for key, value in bundle.items():\n",
    "            if isinstance(value, (list, tuple, np.ndarray)) and len(value) > 0:\n",
    "                if isinstance(value[0], str):  # ë¬¸ìì—´ ë°°ì—´ì´ë©´ ë¼ë²¨ì¼ ê°€ëŠ¥ì„±\n",
    "                    print(f\"[DEBUG] '{key}'ì—ì„œ ë¬¸ìì—´ ë°°ì—´ ë°œê²¬: {len(value)}ê°œ\")\n",
    "                    labels = list(value)\n",
    "                    break\n",
    "    \n",
    "    thresholds = bundle.get(\"thresholds\", 0.5)\n",
    "    print(f\"[DEBUG] ì„ê³„ê°’ íƒ€ì…: {type(thresholds)}\")\n",
    "\n",
    "    # ë¼ë²¨ ì •ë ¬/ê²€ì¦\n",
    "    if hasattr(clf, \"classes_\") and labels is not None:\n",
    "        clf_labels = list(clf.classes_)\n",
    "        if list(labels) != clf_labels:\n",
    "            print(f\"[DEBUG] ë¼ë²¨ ìˆœì„œ ë¶ˆì¼ì¹˜. ë¶„ë¥˜ê¸° ìˆœì„œë¡œ ì¬ì •ë ¬\")\n",
    "            idx_map = {lbl: i for i, lbl in enumerate(labels)}\n",
    "            labels = [lbl for lbl in clf_labels if lbl in idx_map]\n",
    "\n",
    "        # thresholdsê°€ dictë¼ë©´ clf ìˆœì„œì— ë§ì¶° ë°°ì—´ë¡œ ë³€í™˜\n",
    "        if isinstance(thresholds, dict):\n",
    "            thresholds = np.array([float(thresholds.get(lbl, 0.5)) for lbl in labels], dtype=float)\n",
    "\n",
    "    print(f\"[DEBUG] ìµœì¢… ë¼ë²¨ ê°œìˆ˜: {len(labels) if labels else 0}\")\n",
    "    return {\"classifier\": clf, \"mlb\": mlb, \"labels\": labels, \"thresholds\": thresholds}\n",
    "\n",
    "def inspect_model_bundle(pkl_path: str):\n",
    "    \"\"\"ëª¨ë¸ ë²ˆë“¤ êµ¬ì¡°ë¥¼ ìì„¸íˆ ë¶„ì„í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ëª¨ë¸ ë²ˆë“¤ êµ¬ì¡° ë¶„ì„\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        obj = joblib.load(pkl_path)\n",
    "        print(f\"ë©”ì¸ ê°ì²´ íƒ€ì…: {type(obj)}\")\n",
    "        \n",
    "        if isinstance(obj, dict):\n",
    "            print(f\"ë”•ì…”ë„ˆë¦¬ í‚¤ ê°œìˆ˜: {len(obj)}\")\n",
    "            for key, value in obj.items():\n",
    "                print(f\"  '{key}': {type(value)}\")\n",
    "                \n",
    "                # ë¼ë²¨ ê´€ë ¨ ì†ì„± í™•ì¸\n",
    "                if hasattr(value, 'classes_'):\n",
    "                    print(f\"    â””â”€ classes_: {len(value.classes_)}ê°œ ({type(value.classes_[0]) if len(value.classes_) > 0 else 'empty'})\")\n",
    "                    if len(value.classes_) > 0:\n",
    "                        print(f\"       ìƒ˜í”Œ: {list(value.classes_[:3])}...\")\n",
    "                \n",
    "                # ë°°ì—´/ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ë‚´ìš© í™•ì¸\n",
    "                if isinstance(value, (list, tuple, np.ndarray)):\n",
    "                    print(f\"    â””â”€ ê¸¸ì´: {len(value)}\")\n",
    "                    if len(value) > 0:\n",
    "                        print(f\"       íƒ€ì…: {type(value[0])}\")\n",
    "                        if isinstance(value[0], str):\n",
    "                            print(f\"       ìƒ˜í”Œ: {list(value[:3])}...\")\n",
    "        \n",
    "        else:\n",
    "            print(\"ë‹¨ì¼ ê°ì²´ (ë¹„ë”•ì…”ë„ˆë¦¬)\")\n",
    "            if hasattr(obj, 'classes_'):\n",
    "                print(f\"  classes_: {len(obj.classes_)}ê°œ\")\n",
    "                print(f\"  ìƒ˜í”Œ: {list(obj.classes_[:3])}...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ë¶„ì„ ì‹¤íŒ¨: {e}\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "\n",
    "def create_manual_labels():\n",
    "    \"\"\"ìˆ˜ë™ìœ¼ë¡œ í–¥ìˆ˜ ì–´ì½”ë“œ ë¼ë²¨ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ (ì°¸ê³ ìš©)\"\"\"\n",
    "    \n",
    "    # ì¼ë°˜ì ì¸ í–¥ìˆ˜ ì–´ì½”ë“œë“¤ (45ê°œ í´ë˜ìŠ¤ ì˜ˆì‹œ)\n",
    "    fragrance_accords = [\n",
    "        \"Citrus\", \"Fresh\", \"Aquatic\", \"Green\", \"Herbal\",           # 0-4: ìƒì¾Œí•œ ê³„ì—´\n",
    "        \"Floral\", \"Rose\", \"Jasmine\", \"Lily\", \"Peony\",             # 5-9: í”Œë¡œëŸ´ ê³„ì—´  \n",
    "        \"Fruity\", \"Apple\", \"Pear\", \"Peach\", \"Berry\",              # 10-14: ê³¼ì¼ ê³„ì—´\n",
    "        \"Spicy\", \"Pepper\", \"Cinnamon\", \"Ginger\", \"Cardamom\",      # 15-19: ìŠ¤íŒŒì´ì‹œ ê³„ì—´\n",
    "        \"Woody\", \"Sandalwood\", \"Cedar\", \"Pine\", \"Vetiver\",        # 20-24: ìš°ë”” ê³„ì—´\n",
    "        \"Oriental\", \"Amber\", \"Incense\", \"Oud\", \"Patchouli\",       # 25-29: ì˜¤ë¦¬ì—”íƒˆ ê³„ì—´\n",
    "        \"Gourmand\", \"Vanilla\", \"Caramel\", \"Chocolate\", \"Coffee\",  # 30-34: êµ¬ë¥´ë§ ê³„ì—´\n",
    "        \"Musk\", \"Animalic\", \"Leather\", \"Tobacco\", \"Smoky\",        # 35-39: ê¹Šì€ ê³„ì—´\n",
    "        \"Powdery\", \"Clean\", \"Soap\", \"Marine\", \"Ozonic\"            # 40-44: íŒŒìš°ë”ë¦¬/í´ë¦° ê³„ì—´\n",
    "    ]\n",
    "    \n",
    "    return fragrance_accords[:45]  # ì •í™•íˆ 45ê°œë¡œ ë§ì¶¤\n",
    "\n",
    "# --------- 2) ì„ë² ë” ë¡œë“œ ----------\n",
    "def load_embedder():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(HF_MODEL)\n",
    "    base_model = AutoModel.from_pretrained(HF_MODEL).to(DEVICE).eval()\n",
    "    return tokenizer, base_model\n",
    "\n",
    "@torch.inference_mode()\n",
    "def encode_texts(texts: List[str], tokenizer, base_model, max_len: int = MAX_LEN, batch_size: int = BATCH_SZ) -> np.ndarray:\n",
    "    out = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        enc = tokenizer(\n",
    "            batch, padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\"\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        last = base_model(**enc).last_hidden_state        # (B, L, H)\n",
    "        mask = enc[\"attention_mask\"].unsqueeze(-1)        # (B, L, 1)\n",
    "        summed = (last * mask).sum(dim=1)                 \n",
    "        counts = mask.sum(dim=1).clamp(min=1)             \n",
    "        emb = summed / counts                             \n",
    "        out.append(emb.detach().cpu().numpy())\n",
    "    return np.vstack(out)\n",
    "\n",
    "# --------- 3) í™•ë¥  í‘œì¤€í™” ----------\n",
    "def _standardize_proba(proba) -> np.ndarray:\n",
    "    if isinstance(proba, list):\n",
    "        cols = []\n",
    "        for p in proba:\n",
    "            p = np.asarray(p)\n",
    "            if p.ndim == 2 and p.shape[1] > 1:\n",
    "                cols.append(p[:, 1])\n",
    "            else:\n",
    "                cols.append(p.ravel())\n",
    "        proba = np.column_stack(cols)\n",
    "    proba = np.asarray(proba)\n",
    "    if proba.ndim == 1:\n",
    "        proba = proba.reshape(1, -1)\n",
    "    return proba\n",
    "\n",
    "# --------- 4) ì„ê³„ê°’ ì ìš© (ìˆ˜ì •ë³¸) ----------\n",
    "def apply_thresholds(p: np.ndarray, labels: List[str], thresholds) -> List[Tuple[str, float]]:\n",
    "    n = len(p)\n",
    "    \n",
    "    # labels ê²€ì¦ ë° ì²˜ë¦¬\n",
    "    if labels is None or len(labels) == 0:\n",
    "        print(f\"[WARN] labelsê°€ ì—†ìŒ. ì¸ë±ìŠ¤ ë²ˆí˜¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤ (len(p)={n})\")\n",
    "        labels = [f\"class_{i}\" for i in range(n)]\n",
    "    elif len(labels) != n:\n",
    "        print(f\"[WARN] len(p)={n}, len(labels)={len(labels)} â†’ ê¸¸ì´ ë§ì¶¤\")\n",
    "        if len(labels) < n:\n",
    "            # labelsê°€ ë¶€ì¡±í•˜ë©´ ì¸ë±ìŠ¤ë¡œ ì±„ì›€\n",
    "            labels = list(labels) + [f\"class_{i}\" for i in range(len(labels), n)]\n",
    "        else:\n",
    "            # labelsê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ìë¦„\n",
    "            labels = labels[:n]\n",
    "\n",
    "    # thresholds ì²˜ë¦¬\n",
    "    if isinstance(thresholds, dict) and labels is not None:\n",
    "        thr_arr = np.array([float(thresholds.get(lbl, 0.5)) for lbl in labels], dtype=float)\n",
    "    elif hasattr(thresholds, \"__len__\") and not isinstance(thresholds, (str, bytes)):\n",
    "        thr_arr = np.array(thresholds, dtype=float)\n",
    "        if len(thr_arr) != n:\n",
    "            thr_arr = np.full(n, float(np.median(thresholds) if len(thresholds) else 0.5), dtype=float)\n",
    "    else:\n",
    "        thr_arr = np.full(n, float(thresholds), dtype=float)\n",
    "\n",
    "    # ì„ê³„ê°’ ì ìš©\n",
    "    idx = np.where(p >= thr_arr)[0]\n",
    "    \n",
    "    # ì•ˆì „í•œ ì¸ë±ìŠ¤ ì ‘ê·¼\n",
    "    names = []\n",
    "    vals = []\n",
    "    for i in idx:\n",
    "        if i < len(labels):\n",
    "            names.append(labels[i])\n",
    "            vals.append(float(p[i]))\n",
    "    \n",
    "    return sorted(list(zip(names, vals)), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# --------- 5) ì˜ˆì¸¡ í´ë˜ìŠ¤ ----------\n",
    "class TextClassifier:\n",
    "    def __init__(self, model_path: str):\n",
    "        print(f\"[Loading] Model bundle from: {model_path}\")\n",
    "        self.bundle = load_bundle(model_path)\n",
    "        self.clf = self.bundle[\"classifier\"]\n",
    "        self.labels = self.bundle[\"labels\"]\n",
    "        self.thresholds = self.bundle[\"thresholds\"]\n",
    "        \n",
    "        print(f\"[Loading] Embedder: {HF_MODEL}\")\n",
    "        self.tokenizer, self.base_model = load_embedder()\n",
    "        \n",
    "        print(f\"[Loaded] Labels: {len(self.labels) if self.labels else 0}\")\n",
    "        if self.labels and len(self.labels) > 0:\n",
    "            print(f\"[Loaded] Sample labels: {self.labels[:5]}{'...' if len(self.labels) > 5 else ''}\")\n",
    "        else:\n",
    "            print(f\"[WARN] labelsê°€ ì—†ìŒ - ëŸ°íƒ€ì„ì— class_0, class_1... í˜•íƒœë¡œ ìƒì„±ë¨\")\n",
    "        print(f\"[Loaded] Thresholds type: {type(self.thresholds).__name__}\")\n",
    "\n",
    "    def predict_labels(self, texts: List[str], topk: int = 3) -> List[Dict[str, Any]]:\n",
    "        vec = encode_texts(texts, self.tokenizer, self.base_model)\n",
    "        proba = _standardize_proba(self.clf.predict_proba(vec))\n",
    "        results = []\n",
    "        \n",
    "        for i, t in enumerate(texts):\n",
    "            p = proba[i]\n",
    "            n_classes = len(p)\n",
    "            \n",
    "            # labels ì•ˆì „ì„± ê²€ì‚¬\n",
    "            if self.labels is None or len(self.labels) == 0:\n",
    "                current_labels = [f\"class_{j}\" for j in range(n_classes)]\n",
    "                print(f\"[INFO] labels ì—†ìŒ â†’ class_0, class_1, ... ì‚¬ìš©\")\n",
    "            elif len(self.labels) != n_classes:\n",
    "                current_labels = list(self.labels) + [f\"class_{j}\" for j in range(len(self.labels), n_classes)]\n",
    "                if len(current_labels) > n_classes:\n",
    "                    current_labels = current_labels[:n_classes]\n",
    "            else:\n",
    "                current_labels = self.labels\n",
    "            \n",
    "            # TopK ê³„ì‚°\n",
    "            idx_sorted = np.argsort(p)[::-1][:topk]\n",
    "            top_pairs = [(current_labels[j], float(p[j])) for j in idx_sorted if j < len(current_labels)]\n",
    "            \n",
    "            # ì„ê³„ê°’ ì ìš©\n",
    "            passed = apply_thresholds(p, current_labels, self.thresholds)\n",
    "            \n",
    "            results.append({\n",
    "                \"text\": t,\n",
    "                \"topk\": top_pairs,\n",
    "                \"passed_threshold\": passed,\n",
    "                \"all_probs\": p,\n",
    "            })\n",
    "        return results\n",
    "\n",
    "    def predict_one(self, text: str, topk: int = 5) -> Dict[str, Any]:\n",
    "        out = self.predict_labels([text], topk=topk)[0]\n",
    "        print(f\"\\n[Query] {out['text']}\")\n",
    "        print(f\" - TopK : {', '.join(f'{n}({s:.3f})' for n, s in out['topk'])}\")\n",
    "        if out[\"passed_threshold\"]:\n",
    "            print(f\" - Pass : {', '.join(f'{n}({s:.3f})' for n, s in out['passed_threshold'])}\")\n",
    "        else:\n",
    "            print(\" - Pass : (ì„ê³„ê°’ í†µê³¼ ì—†ìŒ â†’ TopK ì°¸ê³ )\")\n",
    "        return out\n",
    "\n",
    "# ============================================\n",
    "# í…ŒìŠ¤íŠ¸ ì½”ë“œ\n",
    "# ============================================\n",
    "\n",
    "def run_tests():\n",
    "    \"\"\"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ í•¨ìˆ˜\"\"\"\n",
    "    \n",
    "    # ë¨¼ì € ëª¨ë¸ êµ¬ì¡° ë¶„ì„\n",
    "    inspect_model_bundle(MODEL_PKL)\n",
    "    \n",
    "    # í–¥ìˆ˜ ê´€ë ¨ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë“¤ (ë©”ì¸ ì–´ì½”ë“œ ì¶”ì¶œìš©)\n",
    "    test_queries = [\n",
    "        # ê³„ì ˆë³„ í–¥ìˆ˜ ì¶”ì²œ\n",
    "        \"ì—¬ë¦„ì— ë¿Œë¦´ë§Œí•œ í–¥ìˆ˜ ì¶”ì²œí•´ì£¼ì„¸ìš”\",\n",
    "        \"ê²¨ìš¸ì— ì–´ìš¸ë¦¬ëŠ” ë”°ëœ»í•œ í–¥ìˆ˜ê°€ ë­ê°€ ìˆì„ê¹Œìš”?\",\n",
    "        \"ë´„ì— ì–´ìš¸ë¦¬ëŠ” í”Œë¡œëŸ´ í–¥ìˆ˜ ì¶”ì²œí•´ì£¼ì„¸ìš”\",\n",
    "        \"ê°€ì„ì— ë¿Œë¦¬ê¸° ì¢‹ì€ ìš°ë””í•œ í–¥ìˆ˜ëŠ”?\",\n",
    "        \n",
    "        # ìƒí™©ë³„ í–¥ìˆ˜ ì¶”ì²œ\n",
    "        \"ë°ì´íŠ¸í•  ë•Œ ë¿Œë¦¬ë©´ ì¢‹ì€ í–¥ìˆ˜ ì¶”ì²œ\",\n",
    "        \"ì§ì¥ì—ì„œ ë¿Œë ¤ë„ ë˜ëŠ” ì€ì€í•œ í–¥ìˆ˜ëŠ”?\",\n",
    "        \"íŒŒí‹°ë‚˜ í´ëŸ½ì— ì–´ìš¸ë¦¬ëŠ” ì„¹ì‹œí•œ í–¥ìˆ˜ ì¶”ì²œ\",\n",
    "        \"ìš´ë™í•  ë•Œ ë¿Œë ¤ë„ ì¢‹ì€ ìƒì¾Œí•œ í–¥ìˆ˜ëŠ”?\",\n",
    "        \n",
    "        # ì„±ë³„/ì—°ë ¹ë³„ ì¶”ì²œ\n",
    "        \"20ëŒ€ ì—¬ì„±ì—ê²Œ ì–´ìš¸ë¦¬ëŠ” í–¥ìˆ˜ ì¶”ì²œí•´ì£¼ì„¸ìš”\",\n",
    "        \"30ëŒ€ ë‚¨ì„±ì´ ì“°ê¸° ì¢‹ì€ í–¥ìˆ˜ëŠ”?\",\n",
    "        \"10ëŒ€ê°€ ì“°ê¸°ì— ë¶€ë‹´ì—†ëŠ” í–¥ìˆ˜ ì¶”ì²œ\",\n",
    "        \"50ëŒ€ ì—¬ì„±ì—ê²Œ ì–´ìš¸ë¦¬ëŠ” ìš°ì•„í•œ í–¥ìˆ˜ëŠ”?\",\n",
    "        \n",
    "        # í–¥ì¡°/ì–´ì½”ë“œë³„ ë¬¸ì˜\n",
    "        \"ì‹œíŠ¸ëŸ¬ìŠ¤ ê³„ì—´ì˜ ìƒí¼í•œ í–¥ìˆ˜ ì¶”ì²œ\",\n",
    "        \"ë°”ë‹ë¼ í–¥ì´ ê°•í•œ ë‹¬ì½¤í•œ í–¥ìˆ˜ëŠ”?\",\n",
    "        \"ë¨¸ìŠ¤í¬ í–¥ì´ ë“¤ì–´ê°„ í–¥ìˆ˜ ì¶”ì²œí•´ì£¼ì„¸ìš”\",\n",
    "        \"ìš°ë”” ê³„ì—´ì˜ ì¤‘í›„í•œ í–¥ìˆ˜ê°€ ë­ê°€ ìˆë‚˜ìš”?\",\n",
    "        \"í”Œë¡œëŸ´ ë¶€ì¼€ í–¥ìˆ˜ ì¤‘ì— ì¶”ì²œí•´ì£¼ì„¸ìš”\",\n",
    "        \n",
    "        # ë¸Œëœë“œ/ê°€ê²©ëŒ€ë³„\n",
    "        \"ìƒ¤ë„¬ í–¥ìˆ˜ ì¤‘ì— ì¸ê¸°ìˆëŠ”ê±° ì¶”ì²œ\",\n",
    "        \"10ë§Œì› ì´í•˜ë¡œ ì‚´ ìˆ˜ ìˆëŠ” ì¢‹ì€ í–¥ìˆ˜ëŠ”?\",\n",
    "        \"ë””ì˜¬ í–¥ìˆ˜ ì¤‘ì— ì—¬ë¦„ìš©ìœ¼ë¡œ ì¢‹ì€ê±´?\",\n",
    "        \"í•™ìƒë„ ì‚´ ìˆ˜ ìˆëŠ” ì €ë ´í•œ í–¥ìˆ˜ ì¶”ì²œ\",\n",
    "        \n",
    "        # íŠ¹ì • í–¥ ì„ í˜¸ë„\n",
    "        \"ì¥ë¯¸í–¥ì´ ë‚˜ëŠ” í–¥ìˆ˜ ì¶”ì²œí•´ì£¼ì„¸ìš”\",\n",
    "        \"ì˜¤ì…˜ ëƒ„ìƒˆê°€ ë‚˜ëŠ” ì‹œì›í•œ í–¥ìˆ˜ëŠ”?\",\n",
    "        \"ê³¼ì¼í–¥ì´ ê°•í•œ í”„ë£¨í‹°í•œ í–¥ìˆ˜ ì¶”ì²œ\",\n",
    "        \"ì»¤í”¼ë‚˜ ì´ˆì½œë¦¿ í–¥ì´ ë‚˜ëŠ” í–¥ìˆ˜ëŠ”?\",\n",
    "        \"í—ˆë¸Œí–¥ì´ ë“¤ì–´ê°„ ìì—°ìŠ¤ëŸ¬ìš´ í–¥ìˆ˜ ì¶”ì²œ\"\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # ë¶„ë¥˜ê¸° ì´ˆê¸°í™”\n",
    "        print(f\"\\nëª¨ë¸ ë¡œë”©ì„ ì‹œë„í•©ë‹ˆë‹¤...\")\n",
    "        classifier = TextClassifier(MODEL_PKL)\n",
    "        \n",
    "        # ë¼ë²¨ì´ ì—†ìœ¼ë©´ ìˆ˜ë™ìœ¼ë¡œ ìƒì„± ì œì•ˆ\n",
    "        if not classifier.labels or len(classifier.labels) == 0:\n",
    "            print(f\"\\nğŸ’¡ ë¼ë²¨ì´ ì—†ìœ¼ë¯€ë¡œ í–¥ìˆ˜ ì–´ì½”ë“œë¥¼ ìˆ˜ë™ìœ¼ë¡œ ë§¤í•‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "            manual_labels = create_manual_labels()\n",
    "            \n",
    "            response = input(f\"ìˆ˜ë™ í–¥ìˆ˜ ì–´ì½”ë“œ ë¼ë²¨ì„ ì‚¬ìš©í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): \").strip().lower()\n",
    "            if response in ['y', 'yes', 'ë„¤', 'ã…‡']:\n",
    "                classifier.labels = manual_labels\n",
    "                print(f\"âœ… í–¥ìˆ˜ ì–´ì½”ë“œ ë¼ë²¨ {len(manual_labels)}ê°œë¥¼ ì ìš©í–ˆìŠµë‹ˆë‹¤.\")\n",
    "                print(f\"ìƒ˜í”Œ: {manual_labels[:5]}...\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"í–¥ìˆ˜ ë©”ì¸ ì–´ì½”ë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ (predict_one)\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # ê°œë³„ í…ŒìŠ¤íŠ¸ (ìƒìœ„ 3ê°œ)\n",
    "        for i, query in enumerate(test_queries[:5]):  # ì²˜ìŒ 5ê°œë§Œ\n",
    "            print(f\"\\n[Test {i+1}]\")\n",
    "            result = classifier.predict_one(query, topk=3)\n",
    "            \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ë°°ì¹˜ í–¥ìˆ˜ ì–´ì½”ë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # ë°°ì¹˜ í…ŒìŠ¤íŠ¸\n",
    "        batch_results = classifier.predict_labels(test_queries[:8], topk=3)\n",
    "        \n",
    "        for i, result in enumerate(batch_results):\n",
    "            print(f\"\\n[Batch {i+1}] {result['text'][:50]}...\")\n",
    "            print(f\"  Top3: {', '.join(f'{n}({s:.3f})' for n, s in result['topk'])}\")\n",
    "            if result['passed_threshold']:\n",
    "                print(f\"  Pass: {', '.join(f'{n}({s:.3f})' for n, s in result['passed_threshold'])}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ë©”ì¸ ì–´ì½”ë“œ ì¶”ì¶œ í†µê³„\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # ì„ê³„ê°’ í†µê³¼ í†µê³„\n",
    "        total_tests = len(batch_results)\n",
    "        passed_tests = sum(1 for r in batch_results if r['passed_threshold'])\n",
    "        \n",
    "        print(f\"ì „ì²´ í…ŒìŠ¤íŠ¸: {total_tests}\")\n",
    "        print(f\"ì„ê³„ê°’ í†µê³¼: {passed_tests}\")\n",
    "        print(f\"ì–´ì½”ë“œ ì¶”ì¶œìœ¨: {passed_tests/total_tests*100:.1f}%\")\n",
    "        \n",
    "        # ì–´ì½”ë“œë³„ í†µê³„\n",
    "        accord_counts = {}\n",
    "        for result in batch_results:\n",
    "            for accord, score in result['passed_threshold']:\n",
    "                accord_counts[accord] = accord_counts.get(accord, 0) + 1\n",
    "        \n",
    "        if accord_counts:\n",
    "            print(\"\\në©”ì¸ ì–´ì½”ë“œë³„ ë“±ì¥ íšŸìˆ˜:\")\n",
    "            for accord, count in sorted(accord_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "                print(f\"  {accord}: {count}íšŒ\")\n",
    "        \n",
    "        print(\"\\ní…ŒìŠ¤íŠ¸ ì™„ë£Œ!\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"[ERROR] ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {MODEL_PKL}\")\n",
    "        print(\"ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”:\")\n",
    "        print(\"1. model.pkl íŒŒì¼ì´ í˜„ì¬ ë””ë ‰í† ë¦¬ì— ìˆëŠ”ì§€ í™•ì¸\")\n",
    "        print(\"2. MODEL_PKL í™˜ê²½ë³€ìˆ˜ë¡œ ì˜¬ë°”ë¥¸ ê²½ë¡œ ì„¤ì •\")\n",
    "        print(\"3. ì˜ˆì‹œ: export MODEL_PKL='/path/to/your/model.pkl'\")\n",
    "        \n",
    "    except ValueError as e:\n",
    "        print(f\"[ERROR] ëª¨ë¸ ë¡œë”© ì˜¤ë¥˜: {str(e)}\")\n",
    "        print(\"model.pkl íŒŒì¼ì˜ êµ¬ì¡°ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\")\n",
    "        print(f\"ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "def interactive_test():\n",
    "    \"\"\"ëŒ€í™”í˜• í…ŒìŠ¤íŠ¸ í•¨ìˆ˜\"\"\"\n",
    "    try:\n",
    "        classifier = TextClassifier(MODEL_PKL)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"í–¥ìˆ˜ ë©”ì¸ ì–´ì½”ë“œ ì¶”ì¶œ - ëŒ€í™”í˜• ëª¨ë“œ\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"í–¥ìˆ˜ ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ ë©”ì¸ ì–´ì½”ë“œ 3ê°œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.\")\n",
    "        print(\"ì˜ˆì‹œ: 'ì—¬ë¦„ì— ë¿Œë¦´ë§Œí•œ í–¥ìˆ˜ ì¶”ì²œ', 'ë°ì´íŠ¸ìš© í–¥ìˆ˜ ì¶”ì²œ' ë“±\")\n",
    "        print(\"ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ë˜ëŠ” 'exit'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.\")\n",
    "        \n",
    "        while True:\n",
    "            query = input(\"\\ní–¥ìˆ˜ ì§ˆë¬¸ ì…ë ¥: \").strip()\n",
    "            if query.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:\n",
    "                break\n",
    "            if not query:\n",
    "                continue\n",
    "                \n",
    "            result = classifier.predict_one(query, topk=3)  # ë©”ì¸ ì–´ì½”ë“œ 3ê°œ\n",
    "            \n",
    "            # ì¶”ê°€ ì •ë³´ í‘œì‹œ\n",
    "            if result[\"passed_threshold\"]:\n",
    "                print(f\"\\nâœ… ì¶”ì¶œëœ ë©”ì¸ ì–´ì½”ë“œ: {len(result['passed_threshold'])}ê°œ\")\n",
    "            else:\n",
    "                print(f\"\\nâš ï¸  ì„ê³„ê°’ì„ ë„˜ëŠ” ì–´ì½”ë“œê°€ ì—†ì–´ ìƒìœ„ 3ê°œë¥¼ ì°¸ê³ í•˜ì„¸ìš”\")\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\ní”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {str(e)}\")\n",
    "\n",
    "# ============================================\n",
    "# ë©”ì¸ ì‹¤í–‰ë¶€\n",
    "# ============================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"í–¥ìˆ˜ ë©”ì¸ ì–´ì½”ë“œ ì¶”ì¶œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # í™˜ê²½ ì •ë³´ ì¶œë ¥\n",
    "    print(f\"Model Path: {MODEL_PKL}\")\n",
    "    print(f\"HF Model: {HF_MODEL}\")\n",
    "    print(f\"Device: {DEVICE}\")\n",
    "    print(f\"Max Length: {MAX_LEN}\")\n",
    "    print(f\"Batch Size: {BATCH_SZ}\")\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ì„ íƒ\n",
    "    import sys\n",
    "    if len(sys.argv) > 1 and sys.argv[1] == \"interactive\":\n",
    "        interactive_test()\n",
    "    else:\n",
    "        run_tests()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
