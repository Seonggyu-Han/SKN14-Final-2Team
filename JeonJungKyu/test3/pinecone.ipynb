{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98011299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ì™„ë£Œ\n",
      "âœ… Pinecone í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ\n",
      "âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ\n",
      "ğŸš€ Perfume ë²¡í„° ì—…ë¡œë“œ ì‹œì‘!\n",
      "\n",
      "ğŸ”¨ ì¸ë±ìŠ¤ 'perfume-vectordb2' ìƒì„± ì¤‘...\n",
      "âœ… ì¸ë±ìŠ¤ 'perfume-vectordb2' ìƒì„± ì™„ë£Œ\n",
      "â³ ì¸ë±ìŠ¤ ì¤€ë¹„ ìƒíƒœ í™•ì¸ ì¤‘...(ìµœëŒ€ 10ì´ˆ)\n",
      "âš ï¸ ì¤€ë¹„ í™•ì¸ íƒ€ì„ì•„ì›ƒ â†’ ê°•ì œ ì§„í–‰\n",
      "ğŸ“– CSV ë¡œë”©: perfume_final_vector.csv\n",
      "ğŸ“Š í–‰ 802ê°œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Document ìƒì„±: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 802/802 [00:00<00:00, 11119.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Document 802ê°œ ìƒì„± ì™„ë£Œ\n",
      "\n",
      "================================================================================\n",
      "ğŸ“‹ Document ìƒ˜í”Œ\n",
      "================================================================================\n",
      "\n",
      "[1] ID: perfume_da4dd656e12bc7dc\n",
      "page_content: ê°•ë ¬í•œ ë§¤í˜¹ì˜ í–¥ê¸°.\n",
      "metadata: {'no': 1, 'brand': 'ê²”ë‘', 'name': 'ë­ìŠ¤ë•… ë“œ ê²”ë‘ ì˜¤ ë“œ í¼í“¸', 'concentration': 'ì˜¤ ë“œ í¼í“¸', 'gender': 'Female', 'sizes': ['75'], 'season_score': 'fall', 'day_night_score': 'day', 'id': 'perfume_da4dd656e12bc7dc'}\n",
      "------------------------------------------------------------\n",
      "\n",
      "[2] ID: perfume_b88e79b3bf4bebd1\n",
      "page_content: ì´ë£¨ì–´ì§ˆ ìˆ˜ ì—†ëŠ” ì‚¬ë‘ì˜ ì „ì„¤ ìŠ¤íŒŒì´ì‹œí•œ ë³µìˆ­ì•„ì™€ í˜ì¶œë¦¬ì˜ ë¹„ë°€ìŠ¤ëŸ¬ìš´ ë§Œë‚¨\n",
      "metadata: {'no': 2, 'brand': 'ê²”ë‘', 'name': 'ë ˆì „ë”ë¦¬ ë¯¸ì¸ ì½” ì˜¤ ë“œ í¼í“¸', 'concentration': 'ì˜¤ ë“œ í¼í“¸', 'gender': 'Female', 'sizes': ['75'], 'season_score': 'winter', 'day_night_score': 'day', 'id': 'perfume_b88e79b3bf4bebd1'}\n",
      "------------------------------------------------------------\n",
      "\n",
      "[3] ID: perfume_486182a360098d83\n",
      "page_content: ììŠ¤ë¯¼ê³¼ ìƒŒë‹¬ìš°ë“œì˜ ì¡°í™”ë¡œ ì´ë£¨ì–´ì§„ ì‹ ì„±í•œ ì‚¬ë‘ì˜ í–¥ê¸°\n",
      "metadata: {'no': 3, 'brand': 'ê²”ë‘', 'name': 'ë ˆì „ë”ë¦¬ ì‚¼ì‚¬ë¼ ì˜¤ ë“œ ëšœì™ˆë ›', 'concentration': 'ì˜¤ ë“œ ëšœì™ˆë ›', 'gender': 'Female', 'sizes': ['75'], 'season_score': 'fall', 'day_night_score': 'day', 'id': 'perfume_486182a360098d83'}\n",
      "------------------------------------------------------------\n",
      "================================================================================\n",
      "\n",
      "ğŸ”„ ì„ë² ë”©(ë°°ì¹˜) ìƒì„±: batch=128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ§® ì„ë² ë”© ë°°ì¹˜: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:12<00:00,  1.74s/it]\n",
      "c:\\Users\\Playdata2\\miniconda3\\envs\\final-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë²¡í„° 802ê°œ ìƒì„± ì™„ë£Œ\n",
      "ğŸ“¤ ì—…ì„œíŠ¸(ë°°ì¹˜): batch=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ ì—…ì„œíŠ¸(batched):  11%|â–ˆ         | 1/9 [00:03<00:25,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â†³ call#1 batch_size=100 (ëˆ„ì  ì„±ê³µ=100, ì‹¤íŒ¨=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ ì—…ì„œíŠ¸(batched):  22%|â–ˆâ–ˆâ–       | 2/9 [00:04<00:13,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â†³ call#2 batch_size=100 (ëˆ„ì  ì„±ê³µ=200, ì‹¤íŒ¨=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ ì—…ì„œíŠ¸(batched):  33%|â–ˆâ–ˆâ–ˆâ–      | 3/9 [00:05<00:09,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â†³ call#3 batch_size=100 (ëˆ„ì  ì„±ê³µ=300, ì‹¤íŒ¨=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ ì—…ì„œíŠ¸(batched):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4/9 [00:06<00:07,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â†³ call#4 batch_size=100 (ëˆ„ì  ì„±ê³µ=400, ì‹¤íŒ¨=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ ì—…ì„œíŠ¸(batched):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5/9 [00:08<00:05,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â†³ call#5 batch_size=100 (ëˆ„ì  ì„±ê³µ=500, ì‹¤íŒ¨=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ ì—…ì„œíŠ¸(batched):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 6/9 [00:09<00:04,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â†³ call#6 batch_size=100 (ëˆ„ì  ì„±ê³µ=600, ì‹¤íŒ¨=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ ì—…ì„œíŠ¸(batched):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 7/9 [00:11<00:02,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â†³ call#7 batch_size=100 (ëˆ„ì  ì„±ê³µ=700, ì‹¤íŒ¨=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ ì—…ì„œíŠ¸(batched):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 8/9 [00:12<00:01,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â†³ call#8 batch_size=100 (ëˆ„ì  ì„±ê³µ=800, ì‹¤íŒ¨=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ ì—…ì„œíŠ¸(batched): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:12<00:00,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â†³ call#9 batch_size=2 (ëˆ„ì  ì„±ê³µ=802, ì‹¤íŒ¨=0)\n",
      "ğŸ“ ì—…ì„œíŠ¸ í˜¸ì¶œìˆ˜: 9\n",
      "âœ… ì—…ì„œíŠ¸ ì™„ë£Œ | ì„±ê³µ: 802  ì‹¤íŒ¨: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š ìµœì¢… ë²¡í„° ìˆ˜: 0\n",
      "ğŸ‰ ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "from typing import Dict, Any, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from openai import OpenAI\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# =========================================\n",
    "# .env ë¡œë“œ\n",
    "# =========================================\n",
    "load_dotenv()\n",
    "\n",
    "# =========================================\n",
    "# ìœ í‹¸\n",
    "# =========================================\n",
    "def _norm(s: Any) -> str:\n",
    "    s = \"\" if s is None else str(s)\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def make_stable_id(brand: str, name: str) -> str:\n",
    "    \"\"\"ë¸Œëœë“œ+ì´ë¦„ ê¸°ë°˜ ì•ˆì •ì  ID\"\"\"\n",
    "    base = f\"{brand.strip()}::{name.strip()}\".lower()\n",
    "    hid = hashlib.sha1(base.encode(\"utf-8\")).hexdigest()[:16]\n",
    "    return f\"perfume_{hid}\"\n",
    "\n",
    "def parse_sizes(sizes_str: str) -> List[str]:\n",
    "    \"\"\"sizes ë¬¸ìì—´ì„ íŒŒì‹±í•´ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜\"\"\"\n",
    "    if pd.isna(sizes_str) or not str(sizes_str).strip() or str(sizes_str).lower() == \"nan\":\n",
    "        return []\n",
    "    \n",
    "    sizes_str = str(sizes_str).strip()\n",
    "    \n",
    "    # [75], [50, 100] ê°™ì€ í˜•íƒœ ì²˜ë¦¬\n",
    "    if sizes_str.startswith('[') and sizes_str.endswith(']'):\n",
    "        try:\n",
    "            # ë¬¸ìì—´ì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ\n",
    "            numbers = re.findall(r'\\d+', sizes_str)\n",
    "            return numbers\n",
    "        except:\n",
    "            return []\n",
    "    \n",
    "    # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ í˜•íƒœë‚˜ ë‹¤ë¥¸ í˜•íƒœ ì²˜ë¦¬\n",
    "    numbers = re.findall(r'\\d+', sizes_str)\n",
    "    return numbers\n",
    "\n",
    "class PerfumeVectorUploader:\n",
    "    def __init__(self):\n",
    "        \"\"\"Pinecone / OpenAI ì´ˆê¸°í™” & ì„¤ì •\"\"\"\n",
    "        self.pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "        self.openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "        if not self.pinecone_api_key:\n",
    "            raise ValueError(\"âŒ PINECONE_API_KEYê°€ .envì— ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        if not self.openai_api_key:\n",
    "            raise ValueError(\"âŒ OPENAI_API_KEYê°€ .envì— ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "        print(\"âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ì™„ë£Œ\")\n",
    "\n",
    "        # Pinecone\n",
    "        try:\n",
    "            self.pc = Pinecone(api_key=self.pinecone_api_key)\n",
    "            print(\"âœ… Pinecone í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"âŒ Pinecone ì´ˆê¸°í™” ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "        # OpenAI\n",
    "        try:\n",
    "            self.openai = OpenAI(api_key=self.openai_api_key)\n",
    "            print(\"âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"âŒ OpenAI ì´ˆê¸°í™” ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "        # ===== ì„¤ì • =====\n",
    "        self.index_name = \"perfume-vectordb2\"\n",
    "        self.dimension = 1536\n",
    "        self.embedding_model = \"text-embedding-3-small\"\n",
    "\n",
    "        self.namespace = \"\"   # í•„ìš” ì‹œ ë¶„ë¦¬\n",
    "        self.embed_batch_size = 128\n",
    "        self.upsert_batch_size = 100\n",
    "\n",
    "    # -------------------------------------\n",
    "    # ì¸ë±ìŠ¤ ì¬ìƒì„± (ì¡´ì¬í•˜ë©´ ì‚­ì œ í›„ ìƒì„±)\n",
    "    # -------------------------------------\n",
    "    def recreate_index(self) -> None:\n",
    "        try:\n",
    "            names = [idx.name for idx in self.pc.list_indexes()]\n",
    "            if self.index_name in names:\n",
    "                print(f\"ğŸ§¨ ì¸ë±ìŠ¤ '{self.index_name}' ì‚­ì œ ì¤‘...\")\n",
    "                self.pc.delete_index(self.index_name)\n",
    "\n",
    "            print(f\"ğŸ”¨ ì¸ë±ìŠ¤ '{self.index_name}' ìƒì„± ì¤‘...\")\n",
    "            self.pc.create_index(\n",
    "                name=self.index_name,\n",
    "                dimension=self.dimension,\n",
    "                metric=\"cosine\",\n",
    "                spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "            )\n",
    "            print(f\"âœ… ì¸ë±ìŠ¤ '{self.index_name}' ìƒì„± ì™„ë£Œ\")\n",
    "            self.wait_until_ready()\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"âŒ ì¸ë±ìŠ¤ ì¬ìƒì„± ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "    def wait_until_ready(self, timeout_sec: int = 10, interval_sec: float = 1.0) -> None:\n",
    "        \"\"\"\n",
    "        ì¸ë±ìŠ¤ê°€ ready ë  ë•Œê¹Œì§€ ì§§ê²Œ í´ë§.\n",
    "        - ê¸°ë³¸: ìµœëŒ€ 10ì´ˆ ë™ì•ˆ 1ì´ˆ ê°„ê²©ìœ¼ë¡œ í™•ì¸\n",
    "        - ê·¸ ì´í›„ì—ëŠ” ê°•ì œë¡œ ì§„í–‰\n",
    "        \"\"\"\n",
    "        print(f\"â³ ì¸ë±ìŠ¤ ì¤€ë¹„ ìƒíƒœ í™•ì¸ ì¤‘...(ìµœëŒ€ {timeout_sec}ì´ˆ)\")\n",
    "        start = time.time()\n",
    "        while True:\n",
    "            try:\n",
    "                desc = self.pc.describe_index(self.index_name)\n",
    "                status = getattr(desc, \"status\", {}) or {}\n",
    "                ready = False\n",
    "                if isinstance(status, dict):\n",
    "                    ready = bool(status.get(\"ready\")) or (status.get(\"state\") == \"Ready\")\n",
    "                if ready:\n",
    "                    print(\"âœ… ì¸ë±ìŠ¤ ì¤€ë¹„ ì™„ë£Œ\")\n",
    "                    return\n",
    "            except Exception:\n",
    "                pass\n",
    "            if time.time() - start > timeout_sec:\n",
    "                print(\"âš ï¸ ì¤€ë¹„ í™•ì¸ íƒ€ì„ì•„ì›ƒ â†’ ê°•ì œ ì§„í–‰\")\n",
    "                return\n",
    "            time.sleep(interval_sec)\n",
    "\n",
    "    # -------------------------------------\n",
    "    # CSV â†’ Document\n",
    "    # -------------------------------------\n",
    "    def parse_score_string(self, score_str: str) -> Optional[str]:\n",
    "        \"\"\"ì ìˆ˜ ë¬¸ìì—´ì—ì„œ ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ í‚¤ë¥¼ ë°˜í™˜\"\"\"\n",
    "        if pd.isna(score_str) or not str(score_str).strip() or str(score_str).lower() == \"nan\":\n",
    "            return None\n",
    "        try:\n",
    "            s = str(score_str).strip()\n",
    "            scores: Dict[str, float] = {}\n",
    "            \n",
    "            # winter(14.2) / spring(24.1) í˜•íƒœ ì²˜ë¦¬\n",
    "            if \"(\" in s and \")\" in s:\n",
    "                pattern = r\"(\\w+)\\s*\\(\\s*([\\d.]+)\\s*\\)\"\n",
    "                for key, val in re.findall(pattern, s):\n",
    "                    try:\n",
    "                        scores[key.strip()] = float(val.strip())\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "            # JSON í˜•íƒœ ì²˜ë¦¬\n",
    "            elif s.startswith(\"{\") and s.endswith(\"}\"):\n",
    "                try:\n",
    "                    d = json.loads(s)\n",
    "                    for k, v in d.items():\n",
    "                        if isinstance(v, str):\n",
    "                            cv = v.replace(\"%\", \"\").strip()\n",
    "                            if cv:\n",
    "                                scores[str(k)] = float(cv)\n",
    "                        elif isinstance(v, (int, float)):\n",
    "                            scores[str(k)] = float(v)\n",
    "                except json.JSONDecodeError:\n",
    "                    pass\n",
    "            \n",
    "            return max(scores, key=scores.get) if scores else None\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def csv_to_documents(self, csv_path: str) -> List[Document]:\n",
    "        if not os.path.exists(csv_path):\n",
    "            raise FileNotFoundError(f\"âŒ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}\")\n",
    "\n",
    "        print(f\"ğŸ“– CSV ë¡œë”©: {csv_path}\")\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"ğŸ“Š í–‰ {len(df)}ê°œ\")\n",
    "\n",
    "        docs: List[Document] = []\n",
    "        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"ğŸ”„ Document ìƒì„±\"):\n",
    "            description = str(row.get(\"description\", \"\")).strip()\n",
    "            if not description or description.lower() == \"nan\":\n",
    "                continue\n",
    "\n",
    "            # ì ìˆ˜ì—ì„œ ìµœê³ ê°’ ì¶”ì¶œ\n",
    "            season_top   = self.parse_score_string(str(row.get(\"season_score\", \"\")))\n",
    "            daynight_top = self.parse_score_string(str(row.get(\"day_night_score\", \"\")))\n",
    "\n",
    "            brand = _norm(row.get(\"brand\", \"\"))\n",
    "            name  = _norm(row.get(\"name\", \"\"))\n",
    "            \n",
    "            # sizesë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ íŒŒì‹±\n",
    "            sizes_list = parse_sizes(str(row.get(\"sizes\", \"\")))\n",
    "\n",
    "            meta: Dict[str, Any] = {\n",
    "                \"no\": int(row.get(\"no\", 0)) if pd.notna(row.get(\"no\")) else 0,\n",
    "                \"brand\": brand,\n",
    "                \"name\": name,\n",
    "                \"concentration\": _norm(row.get(\"concentration\", \"\")),\n",
    "                \"gender\": _norm(row.get(\"gender\", \"\")),\n",
    "                \"sizes\": sizes_list,  # ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì €ì¥\n",
    "            }\n",
    "            \n",
    "            # ìµœê³  ì ìˆ˜ í•­ëª© ì¶”ê°€\n",
    "            if season_top:   \n",
    "                meta[\"season_score\"] = season_top\n",
    "            if daynight_top: \n",
    "                meta[\"day_night_score\"] = daynight_top\n",
    "\n",
    "            # ID ìƒì„±\n",
    "            stable_id = make_stable_id(brand, name)\n",
    "            meta[\"id\"] = stable_id\n",
    "\n",
    "            docs.append(Document(page_content=description, metadata=meta))\n",
    "\n",
    "        print(f\"âœ… Document {len(docs)}ê°œ ìƒì„± ì™„ë£Œ\")\n",
    "        return docs\n",
    "\n",
    "    def show_sample_documents(self, documents: List[Document], n: int = 3) -> None:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"ğŸ“‹ Document ìƒ˜í”Œ\")\n",
    "        print(\"=\" * 80)\n",
    "        for i in range(min(n, len(documents))):\n",
    "            d = documents[i]\n",
    "            print(f\"\\n[{i+1}] ID: {d.metadata['id']}\")\n",
    "            print(f\"page_content: {d.page_content}\")\n",
    "            print(f\"metadata: {d.metadata}\")\n",
    "            print(\"-\" * 60)\n",
    "        print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "    # -------------------------------------\n",
    "    # ë°°ì¹˜ ì„ë² ë”©\n",
    "    # -------------------------------------\n",
    "    def embed_batch(self, texts: List[str]) -> List[List[float]]:\n",
    "        resp = self.openai.embeddings.create(model=self.embedding_model, input=texts)\n",
    "        return [item.embedding for item in resp.data]\n",
    "\n",
    "    def documents_to_vectors_batched(self, docs: List[Document]) -> List[Dict]:\n",
    "        vectors: List[Dict] = []\n",
    "        print(f\"ğŸ”„ ì„ë² ë”©(ë°°ì¹˜) ìƒì„±: batch={self.embed_batch_size}\")\n",
    "        for i in tqdm(range(0, len(docs), self.embed_batch_size), desc=\"ğŸ§® ì„ë² ë”© ë°°ì¹˜\"):\n",
    "            batch_docs = docs[i : i + self.embed_batch_size]\n",
    "            texts = [d.page_content for d in batch_docs]\n",
    "            try:\n",
    "                embs = self.embed_batch(texts)\n",
    "                for d, emb in zip(batch_docs, embs):\n",
    "                    meta = dict(d.metadata)\n",
    "                    meta[\"text\"] = d.page_content  # page_contentë¥¼ textë¡œ ì €ì¥\n",
    "                    vectors.append({\"id\": meta[\"id\"], \"values\": emb, \"metadata\": meta})\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ ì„ë² ë”© ë°°ì¹˜ ì‹¤íŒ¨ (i={i}): {e}\")\n",
    "                continue\n",
    "        print(f\"âœ… ë²¡í„° {len(vectors)}ê°œ ìƒì„± ì™„ë£Œ\")\n",
    "        return vectors\n",
    "\n",
    "    # -------------------------------------\n",
    "    # ì—…ì„œíŠ¸(ë°°ì¹˜)\n",
    "    # -------------------------------------\n",
    "    def upsert_vectors_batched(self, vectors: List[Dict]) -> Tuple[int, int]:\n",
    "        if not vectors:\n",
    "            return 0, 0\n",
    "        index = self.pc.Index(self.index_name)\n",
    "        ok, ng = 0, 0\n",
    "        calls = 0\n",
    "        print(f\"ğŸ“¤ ì—…ì„œíŠ¸(ë°°ì¹˜): batch={self.upsert_batch_size}\")\n",
    "        for i in tqdm(range(0, len(vectors), self.upsert_batch_size), desc=\"ğŸ“¦ ì—…ì„œíŠ¸(batched)\"):\n",
    "            batch = vectors[i : i + self.upsert_batch_size]\n",
    "            try:\n",
    "                res = index.upsert(vectors=batch, namespace=self.namespace)\n",
    "                calls += 1\n",
    "                if hasattr(res, \"upserted_count\") and isinstance(res.upserted_count, int):\n",
    "                    ok += res.upserted_count\n",
    "                else:\n",
    "                    ok += len(batch)\n",
    "            except Exception as e:\n",
    "                ng += len(batch)\n",
    "                print(f\"âš ï¸ ì—…ì„œíŠ¸ ì‹¤íŒ¨ (i={i}): {e}\")\n",
    "                continue\n",
    "            print(f\"   â†³ call#{calls} batch_size={len(batch)} (ëˆ„ì  ì„±ê³µ={ok}, ì‹¤íŒ¨={ng})\")\n",
    "            time.sleep(0.15)\n",
    "        print(f\"ğŸ“ ì—…ì„œíŠ¸ í˜¸ì¶œìˆ˜: {calls}\")\n",
    "        return ok, ng\n",
    "\n",
    "    # -------------------------------------\n",
    "    # ì‹¤í–‰\n",
    "    # -------------------------------------\n",
    "    def run(self, csv_path: str) -> None:\n",
    "        print(\"ğŸš€ Perfume ë²¡í„° ì—…ë¡œë“œ ì‹œì‘!\\n\")\n",
    "\n",
    "        # (1) ì¸ë±ìŠ¤ ì¬ìƒì„±: ì¡´ì¬í•˜ë©´ ì‚­ì œ â†’ ìƒˆë¡œ ìƒì„±\n",
    "        self.recreate_index()\n",
    "\n",
    "        # (2) CSVâ†’Documents\n",
    "        docs = self.csv_to_documents(csv_path)\n",
    "        if not docs:\n",
    "            print(\"âŒ ë³€í™˜í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return\n",
    "        self.show_sample_documents(docs)\n",
    "\n",
    "        # (3) Documentsâ†’Vectors (ë°°ì¹˜ ì„ë² ë”©)\n",
    "        vectors = self.documents_to_vectors_batched(docs)\n",
    "        if not vectors:\n",
    "            print(\"âŒ ìƒì„±í•  ë²¡í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return\n",
    "\n",
    "        # (4) Upsert (ë°°ì¹˜)\n",
    "        ok, ng = self.upsert_vectors_batched(vectors)\n",
    "        print(f\"âœ… ì—…ì„œíŠ¸ ì™„ë£Œ | ì„±ê³µ: {ok}  ì‹¤íŒ¨: {ng}\")\n",
    "\n",
    "        # (5) ìµœì¢… í†µê³„\n",
    "        try:\n",
    "            idx = self.pc.Index(self.index_name)\n",
    "            stats = idx.describe_index_stats()\n",
    "            after = stats.get(\"total_vector_count\", 0)\n",
    "            print(f\"\\nğŸ“Š ìµœì¢… ë²¡í„° ìˆ˜: {after}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ ìµœì¢… í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "        print(\"ğŸ‰ ì™„ë£Œ!\")\n",
    "\n",
    "# =========================================\n",
    "# ë©”ì¸\n",
    "# =========================================\n",
    "def main():\n",
    "    csv_file = \"perfume_final_vector.csv\"\n",
    "    try:\n",
    "        app = PerfumeVectorUploader()\n",
    "        app.run(csv_file)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
